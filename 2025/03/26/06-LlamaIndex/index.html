<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="06-LlamaIndex, Blog">
    <meta name="description" content="1 å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼SDKï¼šSoftware Development Kit**ï¼Œå®ƒæ˜¯ä¸€ç»„è½¯ä»¶å·¥å…·å’Œèµ„æºçš„é›†åˆï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…åˆ›å»ºã€æµ‹è¯•ã€éƒ¨ç½²å’Œç»´æŠ¤åº”ç”¨ç¨‹åºæˆ–è½¯ä»¶ã€‚
æ‰€æœ‰å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰çš„æ ¸å¿ƒä»·å€¼ï¼Œéƒ½æ˜¯é™ä½å¼€å‘ã€ç»´æŠ¤æˆæœ¬ã€‚
å¤§è¯­è¨€">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>06-LlamaIndex | Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>Contact</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Blog</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/TangCharlotte/AI-Classes" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/TangCharlotte/AI-Classes" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">06-LlamaIndex</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2025-03-26
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        
        <!-- ä»£ç å—æŠ˜è¡Œ -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="1-å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼"><a href="#1-å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼" class="headerlink" title="1 å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼"></a>1 å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼</h2><p><em>SDK<strong>ï¼š</strong>Software Development Kit**ï¼Œå®ƒæ˜¯ä¸€ç»„è½¯ä»¶å·¥å…·å’Œèµ„æºçš„é›†åˆï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…åˆ›å»ºã€æµ‹è¯•ã€éƒ¨ç½²å’Œç»´æŠ¤åº”ç”¨ç¨‹åºæˆ–è½¯ä»¶ã€‚</em></p>
<p>æ‰€æœ‰å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰çš„æ ¸å¿ƒä»·å€¼ï¼Œéƒ½æ˜¯é™ä½å¼€å‘ã€ç»´æŠ¤æˆæœ¬ã€‚</p>
<p>å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼ï¼Œæ˜¯è®©å¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°å¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ä¸»è¦æä¾›ä¸¤ç±»å¸®åŠ©ï¼š</p>
<ol>
<li>ç¬¬ä¸‰æ–¹èƒ½åŠ›æŠ½è±¡ã€‚æ¯”å¦‚ LLMã€å‘é‡æ•°æ®åº“ã€æœç´¢æ¥å£ç­‰</li>
<li>å¸¸ç”¨å·¥å…·ã€æ–¹æ¡ˆå°è£…</li>
<li>åº•å±‚å®ç°å°è£…ã€‚æ¯”å¦‚æµå¼æ¥å£ã€è¶…æ—¶é‡è¿ã€å¼‚æ­¥ä¸å¹¶è¡Œç­‰</li>
</ol>
<p>å¥½çš„å¼€å‘æ¡†æ¶ï¼Œéœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p>
<ol>
<li>å¯é æ€§ã€é²æ£’æ€§é«˜</li>
<li>å¯ç»´æŠ¤æ€§é«˜</li>
<li>å¯æ‰©å±•æ€§é«˜</li>
<li>å­¦ä¹ æˆæœ¬ä½</li>
</ol>
<p>ä¸¾äº›é€šä¿—çš„ä¾‹å­ï¼š</p>
<ul>
<li>ä¸å¤–éƒ¨åŠŸèƒ½è§£ä¾èµ–<ul>
<li>æ¯”å¦‚å¯ä»¥éšæ„æ›´æ¢ LLM è€Œä¸ç”¨å¤§é‡é‡æ„ä»£ç </li>
<li>æ›´æ¢ä¸‰æ–¹å·¥å…·ä¹ŸåŒç†</li>
</ul>
</li>
<li>ç»å¸¸å˜çš„éƒ¨åˆ†è¦åœ¨å¤–éƒ¨ç»´æŠ¤è€Œä¸æ˜¯æ”¾åœ¨ä»£ç é‡Œ<ul>
<li>æ¯”å¦‚ Prompt æ¨¡æ¿</li>
</ul>
</li>
<li>å„ç§ç¯å¢ƒä¸‹éƒ½é€‚ç”¨<ul>
<li>æ¯”å¦‚çº¿ç¨‹å®‰å…¨</li>
</ul>
</li>
<li>æ–¹ä¾¿è°ƒè¯•å’Œæµ‹è¯•<ul>
<li>è‡³å°‘è¦èƒ½æ„Ÿè§‰åˆ°ç”¨äº†æ¯”ä¸ç”¨æ–¹ä¾¿å§</li>
<li>åˆæ³•çš„è¾“å…¥ä¸ä¼šå¼•å‘æ¡†æ¶å†…éƒ¨çš„æŠ¥é”™</li>
</ul>
</li>
</ul>
<p><strong>åˆ’é‡ç‚¹ï¼š</strong>é€‰å¯¹äº†æ¡†æ¶ï¼Œäº‹åŠåŠŸå€ï¼›åä¹‹ï¼Œäº‹å€åŠŸåŠã€‚</p>
<p><strong>ä»€ä¹ˆæ˜¯</strong> <strong>SDK</strong><strong>?</strong> <a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/what-is/sdk/">https://aws.amazon.com/cn/what-is/sdk/</a> <strong>SDK å’Œ</strong> <strong>API</strong> <strong>çš„åŒºåˆ«æ˜¯ä»€ä¹ˆ?</strong> <a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/">https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/</a></p>
<p><strong>ğŸŒ° ä¸¾ä¸ªä¾‹å­ï¼šä½¿ç”¨</strong> <strong>SDK****ï¼Œ4 è¡Œä»£ç å®ç°ä¸€ä¸ªç®€æ˜“çš„ RAG ç³»ç»Ÿ</strong></p>
<p>è¿è¡Œæœ¬è¯¾ä»£ç å‰ï¼Œè¯·å…ˆé‡å¯ä¸€ä¸‹ kernelï¼Œä»¥é‡ç½®æ‰€æœ‰é…ç½®ã€‚ </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">!pip install --upgrade llama-index</span><br><span class="line"></span><br><span class="line">from llama_index.core import VectorStoreIndex<span class="punctuation">,</span> SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line"></span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line"></span><br><span class="line">response = query_engine.query(<span class="string">&quot;llama2æœ‰å¤šå°‘å‚æ•°&quot;</span>)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 ranges in scale from 7 billion to 70 billion parameters.</p>
</blockquote>
<h2 id="2-LlamaIndex-ä»‹ç»"><a href="#2-LlamaIndex-ä»‹ç»" class="headerlink" title="2 LlamaIndex ä»‹ç»"></a>2 LlamaIndex ä»‹ç»</h2><p><em>ã€Œ LlamaIndex is a framework for building context-augmented</em> <em>LLM</em> <em>applications. Context augmentation refers to any use case that applies</em> <em>LLMs</em> <em>on top of your private or domain-specific data. ã€</em></p>
<p>LlamaIndex æ˜¯ä¸€ä¸ªä¸ºå¼€å‘ã€Œä¸Šä¸‹æ–‡å¢å¼ºã€çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼ˆä¹Ÿå°±æ˜¯ SDKï¼‰ã€‚<strong>ä¸Šä¸‹æ–‡å¢å¼º</strong>ï¼Œæ³›æŒ‡ä»»ä½•åœ¨ç§æœ‰æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®åŸºç¡€ä¸Šåº”ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š</p>
<img src="/2025/03/26/06-LlamaIndex/1742964772814-2.png" class="">

<ul>
<li>Question-Answering Chatbots (ä¹Ÿå°±æ˜¯ RAG)</li>
<li>Document Understanding and Extraction ï¼ˆæ–‡æ¡£ç†è§£ä¸ä¿¡æ¯æŠ½å–ï¼‰</li>
<li>Autonomous Agents that can perform research and take actions ï¼ˆæ™ºèƒ½ä½“åº”ç”¨ï¼‰</li>
</ul>
<p>LlamaIndex æœ‰ Python å’Œ Typescript ä¸¤ä¸ªç‰ˆæœ¬ï¼ŒPython ç‰ˆçš„æ–‡æ¡£ç›¸å¯¹æ›´å®Œå–„ã€‚</p>
<ul>
<li>Python æ–‡æ¡£åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a></li>
<li>Python API æ¥å£æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/">https://docs.llamaindex.ai/en/stable/api_reference/</a></li>
<li>TS æ–‡æ¡£åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://ts.llamaindex.ai/">https://ts.llamaindex.ai/</a></li>
<li>TS API æ¥å£æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://ts.llamaindex.ai/api/">https://ts.llamaindex.ai/api/</a></li>
</ul>
<p>LlamaIndex æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼ŒGithub é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/run-llama">https://github.com/run-llama</a></p>
<h3 id="2-1-LlamaIndex-çš„æ ¸å¿ƒæ¨¡å—"><a href="#2-1-LlamaIndex-çš„æ ¸å¿ƒæ¨¡å—" class="headerlink" title="2.1 LlamaIndex çš„æ ¸å¿ƒæ¨¡å—"></a>2.1 LlamaIndex çš„æ ¸å¿ƒæ¨¡å—</h3><img src="/2025/03/26/06-LlamaIndex/1742964772814-1.png" class="">

<h3 id="2-2-å®‰è£…-LlamaIndex"><a href="#2-2-å®‰è£…-LlamaIndex" class="headerlink" title="2.2 å®‰è£… LlamaIndex"></a>2.2 å®‰è£… LlamaIndex</h3><ol>
<li>Python</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index</span><br></pre></td></tr></table></figure>

<ol>
<li>Typescript</li>
</ol>
<p>é€šè¿‡ npm å®‰è£… npm install llamaindex é€šè¿‡ yarn å®‰è£… yarn add llamaindex é€šè¿‡ pnpm å®‰è£… pnpm add llamaindex</p>
<p>æœ¬è¯¾ç¨‹ä»¥ Python ç‰ˆä¸ºä¾‹è¿›è¡Œè®²è§£ã€‚</p>
<h2 id="3-æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰"><a href="#3-æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰" class="headerlink" title="3 æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰"></a>3 æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰</h2><h3 id="3-1-åŠ è½½æœ¬åœ°æ•°æ®"><a href="#3-1-åŠ è½½æœ¬åœ°æ•°æ®" class="headerlink" title="3.1 åŠ è½½æœ¬åœ°æ•°æ®"></a>3.1 åŠ è½½æœ¬åœ°æ•°æ®</h3><p><code>SimpleDirectoryReader</code> æ˜¯ä¸€ä¸ªç®€å•çš„æœ¬åœ°æ–‡ä»¶åŠ è½½å™¨ã€‚å®ƒä¼šéå†æŒ‡å®šç›®å½•ï¼Œå¹¶æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½æ–‡ä»¶ï¼ˆ<strong>æ–‡æœ¬å†…å®¹</strong>ï¼‰ã€‚</p>
<p>æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š</p>
<ul>
<li><code>.csv</code> - comma-separated values</li>
<li><code>.docx</code> - Microsoft Word</li>
<li><code>.epub</code> - EPUB ebook format</li>
<li><code>.hwp</code> - Hangul Word Processor</li>
<li><code>.ipynb</code> - Jupyter Notebook</li>
<li><code>.jpeg</code>, <code>.jpg</code> - JPEG image</li>
<li><code>.mbox</code> - MBOX email archive</li>
<li><code>.md</code> - Markdown</li>
<li><code>.mp3</code>, <code>.mp4</code> - audio and video</li>
<li><code>.pdf</code> - Portable Document Format</li>
<li><code>.png</code> - Portable Network Graphics</li>
<li><code>.ppt</code>, <code>.pptm</code>, <code>.pptx</code> - Microsoft PowerPoint</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pydantic.v1 <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_json</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;ç”¨äºå±•ç¤ºjsonæ•°æ®&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, <span class="built_in">str</span>):</span><br><span class="line">        obj = json.loads(data)</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(obj, indent=<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">dict</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>):</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(data, indent=<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">issubclass</span>(<span class="built_in">type</span>(data), BaseModel):</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(data.<span class="built_in">dict</span>(), indent=<span class="number">4</span>, ensure_ascii=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_obj</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;ç”¨äºå±•ç¤ºä¸€ç»„å¯¹è±¡&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">            show_json(item)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Input is not a list&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">reader = SimpleDirectoryReader(</span><br><span class="line">        input_dir=<span class="string">&quot;./data&quot;</span>, <span class="comment"># ç›®æ ‡ç›®å½•</span></span><br><span class="line">        recursive=<span class="literal">False</span>, <span class="comment"># æ˜¯å¦é€’å½’éå†å­ç›®å½•</span></span><br><span class="line">        required_exts=[<span class="string">&quot;.pdf&quot;</span>] <span class="comment"># (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶</span></span><br><span class="line">    )</span><br><span class="line">documents = reader.load_data()</span><br><span class="line"></span><br><span class="line">show_json(documents[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>â€‹    â€œid_â€: â€œ892804e2-9a5d-4853-b12d-2abae6621bfeâ€,</p>
<p>â€‹    â€œembeddingâ€: null,</p>
<p>â€‹    â€œmetadataâ€: {</p>
<p>â€‹        â€œpage_labelâ€: â€œ1â€,</p>
<p>â€‹        â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹        â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹        â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹        â€œfile_sizeâ€: 401338,</p>
<p>â€‹        â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹        â€œlast_modified_dateâ€: â€œ2024-06-14â€</p>
<p>â€‹    },</p>
<p>â€‹    â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹        â€œfile_nameâ€,</p>
<p>â€‹        â€œfile_typeâ€,</p>
<p>â€‹        â€œfile_sizeâ€,</p>
<p>â€‹        â€œcreation_dateâ€,</p>
<p>â€‹        â€œlast_modified_dateâ€,</p>
<p>â€‹        â€œlast_accessed_dateâ€</p>
<p>â€‹    ],</p>
<p>â€‹    â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹        â€œfile_nameâ€,</p>
<p>â€‹        â€œfile_typeâ€,</p>
<p>â€‹        â€œfile_sizeâ€,</p>
<p>â€‹        â€œcreation_dateâ€,</p>
<p>â€‹        â€œlast_modified_dateâ€,</p>
<p>â€‹        â€œlast_accessed_dateâ€</p>
<p>â€‹    ],</p>
<p>â€‹    â€œrelationshipsâ€: {},</p>
<p>â€‹    â€œtextâ€: â€œLlama 2: OpenFoundation andFine-Tuned ChatModels\nHugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller\nCynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev\nPunit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich\nYinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra\nIgor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang\nRoss Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang\nAngela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic\nSergey Edunov ThomasScialomâˆ—\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsibledevelopmentof LLMs.\nâˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\nâ€ Second author\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023â€,</p>
<p>â€‹    â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹    â€œstart_char_idxâ€: null,</p>
<p>â€‹    â€œend_char_idxâ€: null,</p>
<p>â€‹    â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹    â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹    â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹    â€œclass_nameâ€: â€œDocumentâ€</p>
<p>}</p>
<p>Llama 2: OpenFoundation andFine-Tuned ChatModels</p>
<p>Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ </p>
<p>Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra</p>
<p>Prajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen</p>
<p>Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller</p>
<p>Cynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou</p>
<p>Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev</p>
<p>Punit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra</p>
<p>Igor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang</p>
<p>Ross Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang</p>
<p>Angela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic</p>
<p>Sergey Edunov ThomasScialomâˆ—</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our</p>
<p>models outperform open-source chat models on most benchmarks we tested, and based on</p>
<p>ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-</p>
<p>source models. We provide a detailed description of our approach to fine-tuning and safety</p>
<p>improvements of Llama 2-Chat in order to enable the community to build on our work and</p>
<p>contribute to the responsibledevelopmentof LLMs.</p>
<p>âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com</p>
<p>â€ Second author</p>
<p>Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023</p>
</blockquote>
<p><strong>æ³¨æ„ï¼š</strong>å¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ç±»æ–‡ä»¶ï¼Œé»˜è®¤ä¸ä¼šè‡ªåŠ¨æå–å…¶ä¸­æ–‡å­—ã€‚å¦‚éœ€æå–ï¼Œå‚è€ƒä¸‹é¢ä»‹ç»çš„ <code>Data Connectors</code>ã€‚</p>
<p>é»˜è®¤çš„ <code>PDFReader</code> æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¢æ–‡ä»¶åŠ è½½å™¨</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install pymupdf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"></span><br><span class="line">reader = SimpleDirectoryReader(</span><br><span class="line">        input_dir=<span class="string">&quot;./data&quot;</span>, <span class="comment"># ç›®æ ‡ç›®å½•</span></span><br><span class="line">        recursive=<span class="literal">False</span>, <span class="comment"># æ˜¯å¦é€’å½’éå†å­ç›®å½•</span></span><br><span class="line">        required_exts=[<span class="string">&quot;.pdf&quot;</span>], <span class="comment"># (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶</span></span><br><span class="line">        file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125; <span class="comment"># æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶åŠ è½½å™¨</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">documents = reader.load_data()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Hugo Touvronâˆ—</p>
<p>Louis Martinâ€ </p>
<p>Kevin Stoneâ€ </p>
<p>Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra</p>
<p>Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen</p>
<p>Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller</p>
<p>Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou</p>
<p>Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev</p>
<p>Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra</p>
<p>Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang</p>
<p>Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang</p>
<p>Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic</p>
<p>Sergey Edunov</p>
<p>Thomas Scialomâˆ—</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our</p>
<p>models outperform open-source chat models on most benchmarks we tested, and based on</p>
<p>our human evaluations for helpfulness and safety, may be a suitable substitute for closed-</p>
<p>source models. We provide a detailed description of our approach to fine-tuning and safety</p>
<p>improvements of Llama 2-Chat in order to enable the community to build on our work and</p>
<p>contribute to the responsible development of LLMs.</p>
<p>âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com</p>
<p>â€ Second author</p>
<p>Contributions for all the authors can be found in Section A.1.</p>
<p>arXiv:2307.09288v2  [cs.CL]  19 Jul 2023</p>
</blockquote>
<p>æ›´å¤šçš„ PDF åŠ è½½å™¨è¿˜æœ‰ <code>SmartPDFLoader</code> å’Œ <code>LlamaParse</code>, äºŒè€…éƒ½æä¾›äº†æ›´ä¸°å¯Œçš„è§£æèƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£æç« èŠ‚ä¸æ®µè½ç»“æ„ç­‰ã€‚ä½†ä¸æ˜¯ 100%å‡†ç¡®ï¼Œå¶æœ‰æ–‡å­—ä¸¢å¤±æˆ–é”™ä½æƒ…å†µï¼Œå»ºè®®æ ¹æ®è‡ªèº«éœ€æ±‚è¯¦ç»†æµ‹è¯•è¯„ä¼°ã€‚</p>
<h3 id="3-2-Data-Connectors"><a href="#3-2-Data-Connectors" class="headerlink" title="3.2 Data Connectors"></a>3.2 Data Connectors</h3><p>ç”¨äºå¤„ç†æ›´ä¸°å¯Œçš„æ•°æ®ç±»å‹ï¼Œå¹¶å°†å…¶è¯»å–ä¸º <code>Document</code> çš„å½¢å¼ï¼ˆtext + metadataï¼‰ã€‚</p>
<p>ä¾‹å¦‚ï¼šåŠ è½½ä¸€ä¸ª<a target="_blank" rel="noopener" href="https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf">é£ä¹¦æ–‡æ¡£</a>ã€‚ï¼ˆé£ä¹¦æ–‡æ¡£ API è®¿é—®æƒé™ç”³è¯·ï¼Œè¯·å‚è€ƒæ­¤<a target="_blank" rel="noopener" href="http://localhost:8888/files/07-llamaindex/%E9%A3%9E%E4%B9%A6%E6%96%87%E6%A1%A3%E7%9B%B8%E5%85%B3%E6%9D%83%E9%99%90%E7%94%B3%E8%AF%B7.pdf?_xsrf=2%7Cae0d2c71%7C210dc1c6729a7a86b8cdd1c4ac1722a3%7C1742464210">è¯´æ˜æ–‡æ¡£</a>ï¼‰</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install llama-index-readers-feishu-docs</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.readers.feishu_docs <span class="keyword">import</span> FeishuDocsReader</span><br><span class="line"></span><br><span class="line"><span class="comment"># è§è¯´æ˜æ–‡æ¡£</span></span><br><span class="line">app_id = <span class="string">&quot;cli_a6f1c0fa1fd9d00b&quot;</span></span><br><span class="line">app_secret = <span class="string">&quot;dMXCTy8DGaty2xn8I858ZbFDFvcqgiep&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf</span></span><br><span class="line"><span class="comment"># é“¾æ¥æœ€åçš„ &quot;FULadzkWmovlfkxSgLPcE4oWnPf&quot; ä¸ºæ–‡æ¡£ ID </span></span><br><span class="line">doc_ids = [<span class="string">&quot;FULadzkWmovlfkxSgLPcE4oWnPf&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰é£ä¹¦æ–‡æ¡£åŠ è½½å™¨</span></span><br><span class="line">loader = FeishuDocsReader(app_id, app_secret)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ–‡æ¡£</span></span><br><span class="line">documents = loader.load_data(document_ids=doc_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ˜¾ç¤ºå‰1000å­—ç¬¦</span></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>AI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆåŸ¹å…»è®¡åˆ’ - AGIClass.ai</p>
<p>ç”± AGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸º ChatGPT æµªæ½®ä¸­çš„è¶…çº§ä¸ªä½“</p>
<p>ä»€ä¹ˆæ˜¯ AI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆï¼Ÿ</p>
<p>ã€ŒAI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆã€ç®€ç§°ã€ŒAI å…¨æ ˆã€ï¼Œæ˜¯ä¸€ä¸ªäººå°±èƒ½å€ŸåŠ© AIï¼Œè®¾è®¡ã€å¼€å‘å’Œè¿è¥åŸºäº AI çš„å¤§æ¨¡å‹åº”ç”¨çš„è¶…çº§ä¸ªä½“ã€‚</p>
<p>AI å…¨æ ˆéœ€è¦æ‡‚ä¸šåŠ¡ã€æ‡‚ AIã€æ‡‚ç¼–ç¨‹ï¼Œä¸€ä¸ªäººå°±æ˜¯ä¸€ä¸ªå›¢é˜Ÿï¼Œå•æªåŒ¹é©¬åˆ›é€ è´¢å¯Œã€‚</p>
<p>åœ¨æŠ€æœ¯å‹å…¬å¸ï¼ŒAI å…¨æ ˆæœ€æ‡‚ AIï¼Œç¬é—´ç«™ä¸ŠæŠ€æœ¯é¡¶å³°ã€‚</p>
<p>åœ¨éæŠ€æœ¯å‹å…¬å¸ï¼ŒAI å…¨æ ˆè¿æ¥å…¶ä»–å‘˜å·¥å’Œ AIï¼Œæå‡æ•´ä¸ªå…¬å¸çš„æ•ˆç‡ã€‚</p>
<p>åœ¨å…¬å¸å¤–ï¼ŒAI å…¨æ ˆæ¥é¡¹ç›®ï¼Œç‹¬ç«‹å¼€å‘å˜ç°å°å·¥å…·ï¼Œèµšå–ä¸°åšå‰¯ä¸šæ”¶å…¥ã€‚</p>
<p>é€‚åˆäººç¾¤</p>
<p>å­¦ä¹ æœ¬è¯¾ç¨‹ï¼Œå¯ä»¥åœ¨ä¸‹è¿°ç›®æ ‡ä¸­ä¸‰é€‰ä¸€ï¼š</p>
<p>æˆä¸º AI å…¨æ ˆï¼šæ‡‚ä¸šåŠ¡ã€æ‡‚ AI ä¹Ÿæ‡‚ç¼–ç¨‹ã€‚å¤§é‡ä½¿ç”¨ AIï¼Œè‡ªå·±å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹ã€‚åŒ…æ‹¬å•†ä¸šåˆ†æã€éœ€æ±‚åˆ†æã€äº§å“è®¾è®¡ã€å¼€å‘ã€æµ‹è¯•ã€å¸‚åœºæ¨å¹¿å’Œè¿è¥ç­‰</p>
<p>æˆä¸ºä¸šåŠ¡å‘ AI å…¨æ ˆï¼šæ‡‚ä¸šåŠ¡ä¹Ÿæ‡‚ AIï¼Œä¸ç¨‹åºå‘˜åˆä½œï¼Œä¸€èµ·å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹</p>
<p>æˆä¸ºç¼–ç¨‹å‘ AI å…¨æ ˆï¼šæ‡‚ç¼–ç¨‹ä¹Ÿæ‡‚ AIï¼Œä¸ä¸šåŠ¡äººå‘˜åˆä½œï¼Œä¸€èµ·å®Œæˆ AI åº”ç”¨ä»ç­–åˆ’ã€å¼€å‘åˆ°è½åœ°çš„å…¨è¿‡ç¨‹</p>
<p>æ‡‚è‡³å°‘ä¸€é—¨ç¼–ç¨‹è¯­è¨€ï¼Œå¹¶æœ‰è¿‡çœŸå®é¡¹ç›®å¼€å‘ç»éªŒçš„è½¯ä»¶å¼€å‘â¼¯ç¨‹å¸ˆã€â¾¼çº§â¼¯ç¨‹å¸ˆã€æŠ€æœ¯æ€»ç›‘ã€ç ”å‘ç»ç†ã€æ¶æ„å¸ˆã€æµ‹è¯•â¼¯ç¨‹å¸ˆã€æ•°æ®å·¥ç¨‹å¸ˆã€è¿ç»´å·¥ç¨‹å¸ˆç­‰ï¼Œå»ºè®®ä»¥ã€ŒAI å…¨æ ˆã€ä¸ºç›®æ ‡ã€‚å³ä¾¿å¯¹å•†ä¸šã€äº§å“ã€å¸‚åœºç­‰çš„å­¦ä¹ è¾¾ä¸åˆ°æœ€ä½³ï¼Œä½†å·²æŒæ¡çš„ç»éªŒå’Œè®¤çŸ¥ä¹Ÿæœ‰åŠ©äºæˆä¸ºæœ‰ç«äº‰åŠ›çš„ã€Œç¼–ç¨‹å‘AI å…¨æ ˆã€ã€‚</p>
<p>ä¸æ‡‚ç¼–ç¨‹çš„äº§å“ç»ç†ã€éœ€æ±‚åˆ†æå¸ˆã€åˆ›ä¸šè€…ã€è€æ¿ã€è§£å†³æ–¹æ¡ˆå·¥ç¨‹å¸ˆã€é¡¹ç›®ç»ç†ã€è¿è¥ã€å¸‚åœºã€é”€å”®ã€è®¾è®¡å¸ˆç­‰ï¼Œå»ºè®®ä¼˜å…ˆé€‰æ‹©ã€Œä¸šåŠ¡å‘ AI å…¨æ ˆã€ä¸ºç›®æ ‡ã€‚åœ¨è¯¾ç¨‹æä¾›çš„æŠ€æœ¯ç¯å¢ƒé‡Œç†é™¶ï¼Œæé«˜æŠ€æœ¯é¢†åŸŸçš„åˆ¤æ–­åŠ›ï¼Œæœªæ¥å¯ä»¥å’ŒæŠ€æœ¯äººå‘˜æ›´æµç•…åœ°æ²Ÿé€šåä½œã€‚å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå¦‚æœèƒ½å–„ç”¨ AI å­¦ä¹ ç¼–ç¨‹ã€è¾…åŠ©ç¼–ç¨‹ï¼Œå°±å¯ä»¥å‘ã€ŒAI å…¨æ ˆã€è¿ˆè¿›ã€‚</p>
<p>XXX</p>
<p>image.png</p>
</blockquote>
<p><strong>æ›´å¤š Data Connectors</strong></p>
<ul>
<li>å†…ç½®çš„<a target="_blank" rel="noopener" href="https://llamahub.ai/l/readers/llama-index-readers-file">æ–‡ä»¶åŠ è½½å™¨</a></li>
<li>è¿æ¥ä¸‰æ–¹æœåŠ¡çš„<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/">æ•°æ®åŠ è½½å™¨</a>ï¼Œä¾‹å¦‚æ•°æ®åº“</li>
<li>æ›´å¤šåŠ è½½å™¨å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://llamahub.ai/">LlamaHub</a> ä¸Šæ‰¾åˆ°</li>
</ul>
<h2 id="4-æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰"><a href="#4-æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰" class="headerlink" title="4 æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰"></a>4 æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰</h2><p>ä¸ºæ–¹ä¾¿æ£€ç´¢ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠ <code>Document</code> åˆ‡åˆ†ä¸º <code>Node</code>ã€‚</p>
<p>åœ¨ LlamaIndex ä¸­ï¼Œ<code>Node</code> è¢«å®šä¹‰ä¸ºä¸€ä¸ªæ–‡æœ¬çš„ã€Œchunkã€ã€‚</p>
<h3 id="4-1-ä½¿ç”¨-TextSplitters-å¯¹æ–‡æœ¬åšåˆ‡åˆ†"><a href="#4-1-ä½¿ç”¨-TextSplitters-å¯¹æ–‡æœ¬åšåˆ‡åˆ†" class="headerlink" title="4.1 ä½¿ç”¨ TextSplitters å¯¹æ–‡æœ¬åšåˆ‡åˆ†"></a>4.1 ä½¿ç”¨ TextSplitters å¯¹æ–‡æœ¬åšåˆ‡åˆ†</h3><p>ä¾‹å¦‚ï¼š<code>TokenTextSplitter</code> æŒ‰æŒ‡å®š token æ•°åˆ‡åˆ†æ–‡æœ¬</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> TokenTextSplitter</span><br><span class="line"></span><br><span class="line">node_parser = TokenTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">100</span>,  <span class="comment"># æ¯ä¸ª chunk çš„æœ€å¤§é•¿åº¦</span></span><br><span class="line">    chunk_overlap=<span class="number">50</span>  <span class="comment"># chunk ä¹‹é—´é‡å é•¿åº¦ </span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes = node_parser.get_nodes_from_documents(</span><br><span class="line">    documents, show_progress=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">show_json(nodes[<span class="number">0</span>])</span><br><span class="line">show_json(nodes[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>â€‹    â€œid_â€: â€œe29fecbc-961b-4881-9bfb-4714d9515b5câ€,</p>
<p>â€‹    â€œembeddingâ€: null,</p>
<p>â€‹    â€œmetadataâ€: {</p>
<p>â€‹        â€œdocument_idâ€: â€œFULadzkWmovlfkxSgLPcE4oWnPfâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œexcluded_embed_metadata_keysâ€: [],</p>
<p>â€‹    â€œexcluded_llm_metadata_keysâ€: [],</p>
<p>â€‹    â€œrelationshipsâ€: {</p>
<p>â€‹        â€œ1â€: {</p>
<p>â€‹            â€œnode_idâ€: â€œ4d2992f6-1cab-440b-af2c-7b74f5f1152câ€,</p>
<p>â€‹            â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹            â€œmetadataâ€: {</p>
<p>â€‹                â€œdocument_idâ€: â€œFULadzkWmovlfkxSgLPcE4oWnPfâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œhashâ€: â€œ4af8c2ff953f76fa1d608b31dc95b87ee24474294c5e34b83f28902032f054afâ€,</p>
<p>â€‹            â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹        },</p>
<p>â€‹        â€œ3â€: {</p>
<p>â€‹            â€œnode_idâ€: â€œ7569cb43-e42b-4081-9a48-0ff8c90d6181â€,</p>
<p>â€‹            â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹            â€œmetadataâ€: {},</p>
<p>â€‹            â€œhashâ€: â€œ654c6cbdd5a23946a84e84e6f3a474de2a442191b2be2d817ba7f04286b1a980â€,</p>
<p>â€‹            â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹        }</p>
<p>â€‹    },</p>
<p>â€‹    â€œtextâ€: â€œAI å¤§æ¨¡å‹å…¨æ ˆå·¥ç¨‹å¸ˆåŸ¹å…»è®¡åˆ’ - AGIClass.ai\n\nç”± AGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸ºâ€,</p>
<p>â€‹    â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹    â€œstart_char_idxâ€: 0,</p>
<p>â€‹    â€œend_char_idxâ€: 76,</p>
<p>â€‹    â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹    â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹    â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹    â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>}</p>
<p>{</p>
<p>â€‹    â€œid_â€: â€œ7569cb43-e42b-4081-9a48-0ff8c90d6181â€,</p>
<p>â€‹    â€œembeddingâ€: null,</p>
<p>â€‹    â€œmetadataâ€: {</p>
<p>â€‹        â€œdocument_idâ€: â€œFULadzkWmovlfkxSgLPcE4oWnPfâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œexcluded_embed_metadata_keysâ€: [],</p>
<p>â€‹    â€œexcluded_llm_metadata_keysâ€: [],</p>
<p>â€‹    â€œrelationshipsâ€: {</p>
<p>â€‹        â€œ1â€: {</p>
<p>â€‹            â€œnode_idâ€: â€œ4d2992f6-1cab-440b-af2c-7b74f5f1152câ€,</p>
<p>â€‹            â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹            â€œmetadataâ€: {</p>
<p>â€‹                â€œdocument_idâ€: â€œFULadzkWmovlfkxSgLPcE4oWnPfâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œhashâ€: â€œ4af8c2ff953f76fa1d608b31dc95b87ee24474294c5e34b83f28902032f054afâ€,</p>
<p>â€‹            â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹        },</p>
<p>â€‹        â€œ2â€: {</p>
<p>â€‹            â€œnode_idâ€: â€œe29fecbc-961b-4881-9bfb-4714d9515b5câ€,</p>
<p>â€‹            â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹            â€œmetadataâ€: {</p>
<p>â€‹                â€œdocument_idâ€: â€œFULadzkWmovlfkxSgLPcE4oWnPfâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œhashâ€: â€œb08e60a1cf7fa55aa8c010d792766208dcbb34e58aeead16dca005eab4e1df8fâ€,</p>
<p>â€‹            â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹        },</p>
<p>â€‹        â€œ3â€: {</p>
<p>â€‹            â€œnode_idâ€: â€œ1d77241c-5d68-47b8-a475-a9793ca3397aâ€,</p>
<p>â€‹            â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹            â€œmetadataâ€: {},</p>
<p>â€‹            â€œhashâ€: â€œ06d6c13287ff7e2f033a1aae487198dbfdec3d954aab0fd9b4866ce833200afbâ€,</p>
<p>â€‹            â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹        }</p>
<p>â€‹    },</p>
<p>â€‹    â€œtextâ€: â€œAGI è¯¾å ‚æ¨å‡ºçš„ç¤¾ç¾¤å‹ä¼šå‘˜åˆ¶è¯¾ç¨‹ï¼Œä¼ æˆå¤§æ¨¡å‹çš„åŸç†ã€åº”ç”¨å¼€å‘æŠ€æœ¯å’Œè¡Œä¸šè®¤çŸ¥ï¼ŒåŠ©ä½ æˆä¸º ChatGPT æµªæ½®ä¸­çš„è¶…çº§ä¸ªä½“\nä»€ä¹ˆæ˜¯ AIâ€,</p>
<p>â€‹    â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹    â€œstart_char_idxâ€: 33,</p>
<p>â€‹    â€œend_char_idxâ€: 100,</p>
<p>â€‹    â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹    â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹    â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹    â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>}</p>
</blockquote>
<p>LlamaIndex æä¾›äº†ä¸°å¯Œçš„ <code>TextSplitter</code>ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><code>SentenceSplitter</code>ï¼šåœ¨åˆ‡åˆ†æŒ‡å®šé•¿åº¦çš„ chunk åŒæ—¶å°½é‡ä¿è¯å¥å­è¾¹ç•Œä¸è¢«åˆ‡æ–­ï¼›</li>
<li><code>CodeSplitter</code>ï¼šæ ¹æ® ASTï¼ˆç¼–è¯‘å™¨çš„æŠ½è±¡å¥æ³•æ ‘ï¼‰åˆ‡åˆ†ä»£ç ï¼Œä¿è¯ä»£ç åŠŸèƒ½ç‰‡æ®µå®Œæ•´ï¼›</li>
<li><code>SemanticSplitterNodeParser</code>ï¼šæ ¹æ®è¯­ä¹‰ç›¸å…³æ€§å¯¹å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºç‰‡æ®µã€‚</li>
</ul>
<h3 id="4-2-ä½¿ç”¨-NodeParsers-å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ"><a href="#4-2-ä½¿ç”¨-NodeParsers-å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ" class="headerlink" title="4.2 ä½¿ç”¨ NodeParsers å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ"></a>4.2 ä½¿ç”¨ NodeParsers å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ</h3><p>ä¾‹å¦‚ï¼š<code>MarkdownNodeParser</code>è§£æ markdown æ–‡æ¡£</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> FlatReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> MarkdownNodeParser</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">md_docs = FlatReader().load_data(Path(<span class="string">&quot;./data/ChatALL.md&quot;</span>))</span><br><span class="line">parser = MarkdownNodeParser()</span><br><span class="line">nodes = parser.get_nodes_from_documents(md_docs)</span><br><span class="line"></span><br><span class="line">show_json(nodes[<span class="number">2</span>])</span><br><span class="line">show_json(nodes[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{    â€œid_â€: â€œ95fdd1ba-f376-423c-8e56-791b959f5427â€,    â€œembeddingâ€: null,    â€œmetadataâ€: {        â€œfilenameâ€: â€œChatALL.mdâ€,        â€œextensionâ€: â€œ.mdâ€,        â€œHeader_2â€: â€œåŠŸèƒ½â€    },    â€œexcluded_embed_metadata_keysâ€: [],    â€œexcluded_llm_metadata_keysâ€: [],    â€œrelationshipsâ€: {        â€œ1â€: {            â€œnode_idâ€: â€œ4a985a3f-cf0f-41bb-b3a4-eda18a1351ecâ€,            â€œnode_typeâ€: â€œ4â€,            â€œmetadataâ€: {                â€œfilenameâ€: â€œChatALL.mdâ€,                â€œextensionâ€: â€œ.mdâ€            },            â€œhashâ€: â€œ45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87eâ€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        },        â€œ2â€: {            â€œnode_idâ€: â€œ7a5e7373-f294-433f-b361-a9051af73938â€,            â€œnode_typeâ€: â€œ1â€,            â€œmetadataâ€: {                â€œfilenameâ€: â€œChatALL.mdâ€,                â€œextensionâ€: â€œ.mdâ€,                â€œHeader_2â€: â€œå±å¹•æˆªå›¾â€            },            â€œhashâ€: â€œf6065ad5e9929bc7ee14e3c4cc2d29c06788501df8887476c30b279ba8ffd594â€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        },        â€œ3â€: {            â€œnode_idâ€: â€œced63c8e-eda5-46e5-9d81-d9140a37ab92â€,            â€œnode_typeâ€: â€œ1â€,            â€œmetadataâ€: {                â€œHeader_2â€: â€œåŠŸèƒ½â€,                â€œHeader_3â€: â€œè¿™æ˜¯ä½ å—ï¼Ÿâ€            },            â€œhashâ€: â€œf54ac07d417fbcbd606e7cdd3de28c30804e2213218dec2e6157d5037a23e289â€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        }    },    â€œtextâ€: â€œåŠŸèƒ½\n\nåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ AI æœºå™¨äººéå¸¸ç¥å¥‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¡Œä¸ºå¯èƒ½æ˜¯éšæœºçš„ï¼Œä¸åŒçš„æœºå™¨äººåœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¹Ÿæœ‰å·®å¼‚ã€‚å¦‚æœä½ æƒ³è·å¾—æœ€ä½³ä½“éªŒï¼Œä¸è¦ä¸€ä¸ªä¸€ä¸ªå°è¯•ã€‚ChatALLï¼ˆä¸­æ–‡åï¼šé½å¨ï¼‰å¯ä»¥æŠŠä¸€æ¡æŒ‡ä»¤åŒæ—¶å‘ç»™å¤šä¸ª AIï¼Œå¸®åŠ©æ‚¨å‘ç°æœ€å¥½çš„å›ç­”ã€‚ä½ éœ€è¦åšçš„åªæ˜¯<a target="_blank" rel="noopener" href="https://github.com/sunner/ChatALL/releases">ä¸‹è½½ã€å®‰è£…</a>å’Œæé—®ã€‚â€,    â€œmimetypeâ€: â€œtext&#x2F;plainâ€,    â€œstart_char_idxâ€: 459,    â€œend_char_idxâ€: 650,    â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,    â€œmetadata_templateâ€: â€œ{key}: {value}â€,    â€œmetadata_seperatorâ€: â€œ\nâ€,    â€œclass_nameâ€: â€œTextNodeâ€ } {    â€œid_â€: â€œced63c8e-eda5-46e5-9d81-d9140a37ab92â€,    â€œembeddingâ€: null,    â€œmetadataâ€: {        â€œfilenameâ€: â€œChatALL.mdâ€,        â€œextensionâ€: â€œ.mdâ€,        â€œHeader_2â€: â€œåŠŸèƒ½â€,        â€œHeader_3â€: â€œè¿™æ˜¯ä½ å—ï¼Ÿâ€    },    â€œexcluded_embed_metadata_keysâ€: [],    â€œexcluded_llm_metadata_keysâ€: [],    â€œrelationshipsâ€: {        â€œ1â€: {            â€œnode_idâ€: â€œ4a985a3f-cf0f-41bb-b3a4-eda18a1351ecâ€,            â€œnode_typeâ€: â€œ4â€,            â€œmetadataâ€: {                â€œfilenameâ€: â€œChatALL.mdâ€,                â€œextensionâ€: â€œ.mdâ€            },            â€œhashâ€: â€œ45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87eâ€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        },        â€œ2â€: {            â€œnode_idâ€: â€œ95fdd1ba-f376-423c-8e56-791b959f5427â€,            â€œnode_typeâ€: â€œ1â€,            â€œmetadataâ€: {                â€œfilenameâ€: â€œChatALL.mdâ€,                â€œextensionâ€: â€œ.mdâ€,                â€œHeader_2â€: â€œåŠŸèƒ½â€            },            â€œhashâ€: â€œ90172566aa1795d0f9ac33c954d0b98fde63bf9176950d0ea38e87e4ab6563edâ€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        },        â€œ3â€: {            â€œnode_idâ€: â€œ4f4d8aeb-30ed-45c5-9292-4dc0edce16beâ€,            â€œnode_typeâ€: â€œ1â€,            â€œmetadataâ€: {                â€œHeader_2â€: â€œåŠŸèƒ½â€,                â€œHeader_3â€: â€œæ”¯æŒçš„ AIâ€            },            â€œhashâ€: â€œ1b2b11abec9fc74b725b6c344f37d44736e8e991a3eebdbcfa4ab682506c7b2eâ€,            â€œclass_nameâ€: â€œRelatedNodeInfoâ€        }    },    â€œtextâ€: â€œè¿™æ˜¯ä½ å—ï¼Ÿ\n\nChatALL çš„å…¸å‹ç”¨æˆ·æ˜¯ï¼š\n\n- ğŸ¤  <strong>å¤§æ¨¡å‹é‡åº¦ç©å®¶</strong> ï¼Œå¸Œæœ›ä»å¤§æ¨¡å‹æ‰¾åˆ°æœ€å¥½çš„ç­”æ¡ˆï¼Œæˆ–è€…æœ€å¥½çš„åˆ›ä½œ\n- ğŸ¤“ <strong>å¤§æ¨¡å‹ç ”ç©¶è€…</strong> ï¼Œç›´è§‚æ¯”è¾ƒå„ç§å¤§æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„ä¼˜åŠ£\n- ğŸ˜ <strong>å¤§æ¨¡å‹åº”ç”¨å¼€å‘è€…</strong> ï¼Œå¿«é€Ÿè°ƒè¯• promptï¼Œå¯»æ‰¾è¡¨ç°æœ€ä½³çš„åŸºç¡€æ¨¡å‹â€,    â€œmimetypeâ€: â€œtext&#x2F;plainâ€,    â€œstart_char_idxâ€: 656,    â€œend_char_idxâ€: 788,    â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,    â€œmetadata_templateâ€: â€œ{key}: {value}â€,    â€œmetadata_seperatorâ€: â€œ\nâ€,    â€œclass_nameâ€: â€œTextNodeâ€ }</p>
</blockquote>
<p>æ›´å¤šçš„ <code>NodeParser</code> åŒ…æ‹¬ <code>HTMLNodeParser</code>ï¼Œ<code>JSONNodeParser</code>ç­‰ç­‰ã€‚</p>
<h2 id="5-ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰"><a href="#5-ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰" class="headerlink" title="5 ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰"></a>5 ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰</h2><p><strong>åŸºç¡€æ¦‚å¿µ</strong>ï¼šåœ¨ã€Œæ£€ç´¢ã€ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œã€Œç´¢å¼•ã€å³<code>index</code>ï¼Œ é€šå¸¸æ˜¯æŒ‡ä¸ºäº†å®ç°å¿«é€Ÿæ£€ç´¢è€Œè®¾è®¡çš„ç‰¹å®šã€Œæ•°æ®ç»“æ„ã€ã€‚</p>
<p>ç´¢å¼•çš„å…·ä½“åŸç†ä¸å®ç°ä¸æ˜¯æœ¬è¯¾ç¨‹çš„æ•™å­¦é‡ç‚¹ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Search_engine_indexing">ä¼ ç»Ÿç´¢å¼•</a>ã€<a target="_blank" rel="noopener" href="https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5">å‘é‡ç´¢å¼•</a></p>
<h3 id="5-1-å‘é‡æ£€ç´¢"><a href="#5-1-å‘é‡æ£€ç´¢" class="headerlink" title="5.1 å‘é‡æ£€ç´¢"></a>5.1 å‘é‡æ£€ç´¢</h3><ol>
<li><code>SimpleVectorStore</code> ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºä¸€ä¸ª Vector Store å¹¶å»ºç´¢å¼•</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> TokenTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½ pdf æ–‡æ¡£</span></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    <span class="string">&quot;./data&quot;</span>, </span><br><span class="line">    required_exts=[<span class="string">&quot;.pdf&quot;</span>],</span><br><span class="line">    file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;</span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰ Node Parser</span></span><br><span class="line">node_parser = TokenTextSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ‡åˆ†æ–‡æ¡£</span></span><br><span class="line">nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ„å»º index</span></span><br><span class="line">index = VectorStoreIndex(nodes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å– retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(</span><br><span class="line">    similarity_top_k=<span class="number">2</span> <span class="comment"># è¿”å›å‰ä¸¤ä¸ªç»“æœ</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€ç´¢</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2æœ‰å¤šå°‘å‚æ•°&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>â€‹    â€œnodeâ€: {</p>
<p>â€‹        â€œid_â€: â€œ4a6537d5-72de-4eec-a6ee-981b44396d79â€,</p>
<p>â€‹        â€œembeddingâ€: null,</p>
<p>â€‹        â€œmetadataâ€: {</p>
<p>â€‹            â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹            â€œfile_sizeâ€: 401338,</p>
<p>â€‹            â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œtotal_pagesâ€: 4,</p>
<p>â€‹            â€œsourceâ€: â€œ4â€</p>
<p>â€‹        },</p>
<p>â€‹        â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œrelationshipsâ€: {</p>
<p>â€‹            â€œ1â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œe1be7502-7883-45cf-986a-0c88ecd7bad1â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œb29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519aâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ2â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ4ffd9047-f4a4-438c-8871-09673a8ac4d2â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œ07108bd9b8afa14b2c31a05dbe825ddf1cf5ca4ddcc8bb87eb57ce03045e7bc7â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ3â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ7f63c496-88da-4db6-8362-a2694772d621â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {},</p>
<p>â€‹                â€œhashâ€: â€œ726b948dfd9e32d525760994bddb61dbfaba8a0d18d12a3e3a4f9504fbc208fdâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            }</p>
<p>â€‹        },</p>
<p>â€‹        â€œtextâ€: â€œan updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.Â§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover\nall scenarios. Therefore, before deploying any applications ofâ€,</p>
<p>â€‹        â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹        â€œstart_char_idxâ€: 752,</p>
<p>â€‹        â€œend_char_idxâ€: 1714,</p>
<p>â€‹        â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹        â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹        â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹        â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œscoreâ€: 0.7901917550666981,</p>
<p>â€‹    â€œclass_nameâ€: â€œNodeWithScoreâ€</p>
<p>}</p>
<p>{</p>
<p>â€‹    â€œnodeâ€: {</p>
<p>â€‹        â€œid_â€: â€œbc33a188-0147-447e-8137-a0caccf05970â€,</p>
<p>â€‹        â€œembeddingâ€: null,</p>
<p>â€‹        â€œmetadataâ€: {</p>
<p>â€‹            â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹            â€œfile_sizeâ€: 401338,</p>
<p>â€‹            â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œtotal_pagesâ€: 4,</p>
<p>â€‹            â€œsourceâ€: â€œ1â€</p>
<p>â€‹        },</p>
<p>â€‹        â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œrelationshipsâ€: {</p>
<p>â€‹            â€œ1â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ1bb809bb-d25f-4e50-b774-ccd7402da25câ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ1â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œ3ebf9b8910125e57c9eea78794c6566c0a85081c97dbf7e83a65e5d791bcda57â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ2â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ4337b7c7-6f45-4d2f-aa31-6def3b07088dâ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ1â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œbf3806ddee17b6020e00e4e722a57980d0c5fc9f813ff437a756c8dd8f44e52fâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ3â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œc29e860f-5f91-4cb2-a9e3-f860a0eb5f7dâ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {},</p>
<p>â€‹                â€œhashâ€: â€œfe31f0ee71493cf072edea470642efdc2be2103b9deb19d3706be19e2d1fef9bâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            }</p>
<p>â€‹        },</p>
<p>â€‹        â€œtextâ€: â€œKoura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialomâˆ—\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute forâ€,</p>
<p>â€‹        â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹        â€œstart_char_idxâ€: 513,</p>
<p>â€‹        â€œend_char_idxâ€: 1464,</p>
<p>â€‹        â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹        â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹        â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹        â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œscoreâ€: 0.7890007200916708,</p>
<p>â€‹    â€œclass_nameâ€: â€œNodeWithScoreâ€</p>
<p>}</p>
</blockquote>
<p>LlamaIndex é»˜è®¤çš„ Embedding æ¨¡å‹æ˜¯ <code>OpenAIEmbedding(model=&quot;text-embedding-ada-002&quot;)</code>ã€‚</p>
<p>å¦‚ä½•æ›¿æ¢æŒ‡å®šçš„ Embedding æ¨¡å‹è§åé¢ç« èŠ‚è¯¦è§£ã€‚</p>
<ol>
<li>ä½¿ç”¨è‡ªå®šä¹‰çš„ Vector Storeï¼Œä»¥ <code>Chroma</code> ä¸ºä¾‹ï¼š</li>
</ol>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install llama-index-vector-stores-chroma</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">if</span> os.environ.get(<span class="string">&#x27;CUR_ENV_IS_STUDENT&#x27;</span>,<span class="string">&#x27;false&#x27;</span>)==<span class="string">&#x27;true&#x27;</span>:</span><br><span class="line">    <span class="built_in">__import__</span>(<span class="string">&#x27;pysqlite3&#x27;</span>)</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    sys.modules[<span class="string">&#x27;sqlite3&#x27;</span>]= sys.modules.pop(<span class="string">&#x27;pysqlite3&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> chromadb.config <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»º Chroma Client</span></span><br><span class="line"><span class="comment"># EphemeralClient åœ¨å†…å­˜åˆ›å»ºï¼›å¦‚æœéœ€è¦å­˜ç›˜ï¼Œå¯ä»¥ä½¿ç”¨ PersistentClient</span></span><br><span class="line">chroma_client = chromadb.EphemeralClient(settings=Settings(allow_reset=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"></span><br><span class="line">chroma_client.reset() <span class="comment"># ä¸ºæ¼”ç¤ºæ–¹ä¾¿ï¼Œå®é™…ä¸ç”¨æ¯æ¬¡ reset</span></span><br><span class="line">chroma_collection = chroma_client.create_collection(<span class="string">&quot;demo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»º Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Storage Context æ˜¯ Vector Store çš„å­˜å‚¨å®¹å™¨ï¼Œç”¨äºå­˜å‚¨æ–‡æœ¬ã€indexã€å‘é‡ç­‰æ•°æ®</span></span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=vector_store)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»º indexï¼šé€šè¿‡ Storage Context å…³è”åˆ°è‡ªå®šä¹‰çš„ Vector Store</span></span><br><span class="line">index = VectorStoreIndex(nodes, storage_context=storage_context)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å– retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€ç´¢</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2æœ‰å¤šå°‘å‚æ•°&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results)</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>{</p>
<p>â€‹    â€œnodeâ€: {</p>
<p>â€‹        â€œid_â€: â€œ4a6537d5-72de-4eec-a6ee-981b44396d79â€,</p>
<p>â€‹        â€œembeddingâ€: null,</p>
<p>â€‹        â€œmetadataâ€: {</p>
<p>â€‹            â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹            â€œfile_sizeâ€: 401338,</p>
<p>â€‹            â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œtotal_pagesâ€: 4,</p>
<p>â€‹            â€œsourceâ€: â€œ4â€</p>
<p>â€‹        },</p>
<p>â€‹        â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œrelationshipsâ€: {</p>
<p>â€‹            â€œ1â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œe1be7502-7883-45cf-986a-0c88ecd7bad1â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œb29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519aâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ2â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ4ffd9047-f4a4-438c-8871-09673a8ac4d2â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œ07108bd9b8afa14b2c31a05dbe825ddf1cf5ca4ddcc8bb87eb57ce03045e7bc7â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ3â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ7f63c496-88da-4db6-8362-a2694772d621â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {},</p>
<p>â€‹                â€œhashâ€: â€œ726b948dfd9e32d525760994bddb61dbfaba8a0d18d12a3e3a4f9504fbc208fdâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            }</p>
<p>â€‹        },</p>
<p>â€‹        â€œtextâ€: â€œan updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.Â§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover\nall scenarios. Therefore, before deploying any applications ofâ€,</p>
<p>â€‹        â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹        â€œstart_char_idxâ€: 752,</p>
<p>â€‹        â€œend_char_idxâ€: 1714,</p>
<p>â€‹        â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹        â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹        â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹        â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œscoreâ€: 0.657386283435787,</p>
<p>â€‹    â€œclass_nameâ€: â€œNodeWithScoreâ€</p>
<p>}</p>
<p>{</p>
<p>â€‹    â€œnodeâ€: {</p>
<p>â€‹        â€œid_â€: â€œbc33a188-0147-447e-8137-a0caccf05970â€,</p>
<p>â€‹        â€œembeddingâ€: null,</p>
<p>â€‹        â€œmetadataâ€: {</p>
<p>â€‹            â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹            â€œfile_sizeâ€: 401338,</p>
<p>â€‹            â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œtotal_pagesâ€: 4,</p>
<p>â€‹            â€œsourceâ€: â€œ1â€</p>
<p>â€‹        },</p>
<p>â€‹        â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œrelationshipsâ€: {</p>
<p>â€‹            â€œ1â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ1bb809bb-d25f-4e50-b774-ccd7402da25câ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ1â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œ3ebf9b8910125e57c9eea78794c6566c0a85081c97dbf7e83a65e5d791bcda57â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ2â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ4337b7c7-6f45-4d2f-aa31-6def3b07088dâ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ1â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œbf3806ddee17b6020e00e4e722a57980d0c5fc9f813ff437a756c8dd8f44e52fâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ3â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œc29e860f-5f91-4cb2-a9e3-f860a0eb5f7dâ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {},</p>
<p>â€‹                â€œhashâ€: â€œfe31f0ee71493cf072edea470642efdc2be2103b9deb19d3706be19e2d1fef9bâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            }</p>
<p>â€‹        },</p>
<p>â€‹        â€œtextâ€: â€œKoura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialomâˆ—\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute forâ€,</p>
<p>â€‹        â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹        â€œstart_char_idxâ€: 513,</p>
<p>â€‹        â€œend_char_idxâ€: 1464,</p>
<p>â€‹        â€œtext_templateâ€: â€œ{metadata_str}\n\n{content}â€,</p>
<p>â€‹        â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹        â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹        â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œscoreâ€: 0.6557053381809197,</p>
<p>â€‹    â€œclass_nameâ€: â€œNodeWithScoreâ€</p>
<p>}</p>
</blockquote>
<h3 id="5-2-æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼"><a href="#5-2-æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼" class="headerlink" title="5.2 æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼"></a>5.2 æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼</h3><p>LlamaIndex å†…ç½®äº†ä¸°å¯Œçš„æ£€ç´¢æœºåˆ¶ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li>å…³é”®å­—æ£€ç´¢<ul>
<li><code>BM25Retriever</code>ï¼šåŸºäº tokenizer å®ç°çš„ BM25 ç»å…¸æ£€ç´¢ç®—æ³•</li>
<li><code>KeywordTableGPTRetriever</code>ï¼šä½¿ç”¨ GPT æå–æ£€ç´¢å…³é”®å­—</li>
<li><code>KeywordTableSimpleRetriever</code>ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ£€ç´¢å…³é”®å­—</li>
<li><code>KeywordTableRAKERetriever</code>ï¼šä½¿ç”¨<code>RAKE</code>ç®—æ³•æå–æ£€ç´¢å…³é”®å­—ï¼ˆæœ‰è¯­è¨€é™åˆ¶ï¼‰</li>
</ul>
</li>
<li>RAG-Fusion <code>QueryFusionRetriever</code></li>
<li>è¿˜æ”¯æŒ <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/">KnowledgeGraph</a>ã€<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever">SQL</a>ã€<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever">Text-to-SQL</a> ç­‰ç­‰</li>
</ul>
<h3 id="5-3-Ingestion-Pipeline-è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹"><a href="#5-3-Ingestion-Pipeline-è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹" class="headerlink" title="5.3 Ingestion Pipeline è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹"></a>5.3 Ingestion Pipeline è‡ªå®šä¹‰æ•°æ®å¤„ç†æµç¨‹</h3><p>LlamaIndex é€šè¿‡ <code>Transformations</code> å®šä¹‰ä¸€ä¸ªæ•°æ®ï¼ˆ<code>Documents</code>ï¼‰çš„å¤šæ­¥å¤„ç†çš„æµç¨‹ï¼ˆPipelineï¼‰ã€‚ è¿™ä¸ª Pipeline çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯ï¼Œ<strong>å®ƒçš„æ¯ä¸ªå­æ­¥éª¤æ˜¯å¯ä»¥ç¼“å­˜ï¼ˆcacheï¼‰çš„</strong>ï¼Œå³å¦‚æœè¯¥å­æ­¥éª¤çš„è¾“å…¥ä¸å¤„ç†æ–¹æ³•ä¸å˜ï¼Œé‡å¤è°ƒç”¨æ—¶ä¼šç›´æ¥ä»ç¼“å­˜ä¸­è·å–ç»“æœï¼Œè€Œæ— éœ€é‡æ–°æ‰§è¡Œè¯¥å­æ­¥éª¤ï¼Œè¿™æ ·å³èŠ‚çœæ—¶é—´ä¹Ÿä¼šèŠ‚çœ token ï¼ˆå¦‚æœå­æ­¥éª¤æ¶‰åŠå¤§æ¨¡å‹è°ƒç”¨ï¼‰ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.start = time.time()</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span><br><span class="line">        <span class="variable language_">self</span>.end = time.time()</span><br><span class="line">        <span class="variable language_">self</span>.interval = <span class="variable language_">self</span>.end - <span class="variable language_">self</span>.start</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;è€—æ—¶ <span class="subst">&#123;self.interval*<span class="number">1000</span>&#125;</span> ms&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.core.extractors <span class="keyword">import</span> TitleExtractor</span><br><span class="line"><span class="keyword">from</span> llama_index.core.ingestion <span class="keyword">import</span> IngestionPipeline</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"><span class="keyword">import</span> nest_asyncio</span><br><span class="line">nest_asyncio.apply() <span class="comment"># åªåœ¨Jupyterç¬”è®°ç¯å¢ƒä¸­éœ€è¦æ­¤æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™</span></span><br><span class="line"></span><br><span class="line">chroma_client.reset() <span class="comment"># ä¸ºæ¼”ç¤ºæ–¹ä¾¿ï¼Œå®é™…ä¸ç”¨æ¯æ¬¡ reset</span></span><br><span class="line">chroma_collection = chroma_client.create_collection(<span class="string">&quot;ingestion_demo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»º Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"></span><br><span class="line">pipeline = IngestionPipeline(</span><br><span class="line">    transformations=[</span><br><span class="line">        SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>), <span class="comment"># æŒ‰å¥å­åˆ‡åˆ†</span></span><br><span class="line">        TitleExtractor(), <span class="comment"># åˆ©ç”¨ LLM å¯¹æ–‡æœ¬ç”Ÿæˆæ ‡é¢˜</span></span><br><span class="line">        OpenAIEmbedding(), <span class="comment"># å°†æ–‡æœ¬å‘é‡åŒ–</span></span><br><span class="line">    ],</span><br><span class="line">    vector_store=vector_store,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    <span class="string">&quot;./data&quot;</span>, </span><br><span class="line">    required_exts=[<span class="string">&quot;.pdf&quot;</span>],</span><br><span class="line">    file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;</span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¡æ—¶</span></span><br><span class="line"><span class="keyword">with</span> Timer():</span><br><span class="line">    <span class="comment"># Ingest directly into a vector db</span></span><br><span class="line">    pipeline.run(documents=documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºç´¢å¼•</span></span><br><span class="line">index = VectorStoreIndex.from_vector_store(vector_store)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è·å– retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€ç´¢</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2æœ‰å¤šå°‘å‚æ•°&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results[:<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3&#x2F;3 [00:00&lt;00:00,  4.56it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5&#x2F;5 [00:01&lt;00:00,  4.97it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5&#x2F;5 [00:01&lt;00:00,  4.78it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4&#x2F;4 [00:00&lt;00:00,  6.43it&#x2F;s]</p>
<p>è€—æ—¶ 6928.267955780029 ms</p>
<p>{</p>
<p>â€‹    â€œnodeâ€: {</p>
<p>â€‹        â€œid_â€: â€œbae00644-0188-4e5e-a0df-4b6342585815â€,</p>
<p>â€‹        â€œembeddingâ€: null,</p>
<p>â€‹        â€œmetadataâ€: {</p>
<p>â€‹            â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹            â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹            â€œfile_sizeâ€: 401338,</p>
<p>â€‹            â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹            â€œtotal_pagesâ€: 4,</p>
<p>â€‹            â€œsourceâ€: â€œ4â€,</p>
<p>â€‹            â€œdocument_titleâ€: â€œResponsible Release and Deployment Strategy for Llama 2 and Llama 2-Chat Modelsâ€</p>
<p>â€‹        },</p>
<p>â€‹        â€œexcluded_embed_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œexcluded_llm_metadata_keysâ€: [</p>
<p>â€‹            â€œfile_nameâ€,</p>
<p>â€‹            â€œfile_typeâ€,</p>
<p>â€‹            â€œfile_sizeâ€,</p>
<p>â€‹            â€œcreation_dateâ€,</p>
<p>â€‹            â€œlast_modified_dateâ€,</p>
<p>â€‹            â€œlast_accessed_dateâ€</p>
<p>â€‹        ],</p>
<p>â€‹        â€œrelationshipsâ€: {</p>
<p>â€‹            â€œ1â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œ9921e324-4f4c-4b9e-92cd-e3aae69a7ca0â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ4â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œb29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519aâ€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ2â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œb38e93ce-156f-4615-bd56-0a51eaa276d2â€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {</p>
<p>â€‹                    â€œfile_pathâ€: â€œ&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_nameâ€: â€œllama2-extracted.pdfâ€,</p>
<p>â€‹                    â€œfile_typeâ€: â€œapplication&#x2F;pdfâ€,</p>
<p>â€‹                    â€œfile_sizeâ€: 401338,</p>
<p>â€‹                    â€œcreation_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œlast_modified_dateâ€: â€œ2024-06-14â€,</p>
<p>â€‹                    â€œtotal_pagesâ€: 4,</p>
<p>â€‹                    â€œsourceâ€: â€œ4â€</p>
<p>â€‹                },</p>
<p>â€‹                â€œhashâ€: â€œ9d81d8fc1b12d06f9209d238abdd84d2e44083be69c925f28443906d62356482â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            },</p>
<p>â€‹            â€œ3â€: {</p>
<p>â€‹                â€œnode_idâ€: â€œc7a204d3-fdd0-42c4-b191-58f43e6bb80eâ€,</p>
<p>â€‹                â€œnode_typeâ€: â€œ1â€,</p>
<p>â€‹                â€œmetadataâ€: {},</p>
<p>â€‹                â€œhashâ€: â€œ84d9afeda9c20e1ede5c3a74fd65c9f1a15c2b124ba73de9369264ecddfbd169â€,</p>
<p>â€‹                â€œclass_nameâ€: â€œRelatedNodeInfoâ€</p>
<p>â€‹            }</p>
<p>â€‹        },</p>
<p>â€‹        â€œtextâ€: â€œLlama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.Â§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023).â€,</p>
<p>â€‹        â€œmimetypeâ€: â€œtext&#x2F;plainâ€,</p>
<p>â€‹        â€œstart_char_idxâ€: 743,</p>
<p>â€‹        â€œend_char_idxâ€: 1569,</p>
<p>â€‹        â€œtext_templateâ€: â€œ[Excerpt from document]\n{metadata_str}\nExcerpt:\nâ€”â€“\n{content}\nâ€”â€“\nâ€,</p>
<p>â€‹        â€œmetadata_templateâ€: â€œ{key}: {value}â€,</p>
<p>â€‹        â€œmetadata_seperatorâ€: â€œ\nâ€,</p>
<p>â€‹        â€œclass_nameâ€: â€œTextNodeâ€</p>
<p>â€‹    },</p>
<p>â€‹    â€œscoreâ€: 0.6505982749190239,</p>
<p>â€‹    â€œclass_nameâ€: â€œNodeWithScoreâ€</p>
<p>}</p>
</blockquote>
<p>æœ¬åœ°ä¿å­˜ <code>IngestionPipeline</code> çš„ç¼“å­˜</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pipeline.persist(<span class="string">&quot;./pipeline_storage&quot;</span>)</span><br><span class="line"></span><br><span class="line">new_pipeline = IngestionPipeline(</span><br><span class="line">    transformations=[</span><br><span class="line">        SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>),</span><br><span class="line">        TitleExtractor(),</span><br><span class="line">        OpenAIEmbedding()</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½ç¼“å­˜</span></span><br><span class="line">new_pipeline.load(<span class="string">&quot;./pipeline_storage&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Timer():</span><br><span class="line">    nodes = new_pipeline.run(documents=documents)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.20it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  3.07it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.67it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.77it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5&#x2F;5 [00:00&lt;00:00,  5.59it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2&#x2F;2 [00:00&lt;00:00,  2.09it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.33it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.86it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.65it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.89it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.12it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.52it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.29it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.42it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.46it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.44it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.31it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.97it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  1.98it&#x2F;s]</p>
<p>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1&#x2F;1 [00:00&lt;00:00,  2.12it&#x2F;s]</p>
<p>è€—æ—¶ 22366.679430007935 ms</p>
</blockquote>
<p>æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥ç”¨è¿œç¨‹çš„ Redis æˆ– MongoDB ç­‰å­˜å‚¨ <code>IngestionPipeline</code> çš„ç¼“å­˜ï¼Œå…·ä½“å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#remote-cache-management">Remote Cache Management</a>ã€‚</p>
<p><code>IngestionPipeline</code> ä¹Ÿæ”¯æŒå¼‚æ­¥å’Œå¹¶å‘è°ƒç”¨ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#async-support">Async Support</a>ã€<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#parallel-processing">Parallel Processing</a>ã€‚</p>
<h3 id="5-4-æ£€ç´¢åå¤„ç†"><a href="#5-4-æ£€ç´¢åå¤„ç†" class="headerlink" title="5.4 æ£€ç´¢åå¤„ç†"></a>5.4 æ£€ç´¢åå¤„ç†</h3><p>LlamaIndex çš„ <code>Node Postprocessors</code> æä¾›äº†ä¸€ç³»åˆ—æ£€ç´¢åå¤„ç†æ¨¡å—ã€‚</p>
<p>ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒæ¨¡å‹å¯¹æ£€ç´¢åçš„ <code>Nodes</code> åšé‡æ’åº</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è·å– retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€ç´¢</span></span><br><span class="line">nodes = vector_retriever.retrieve(<span class="string">&quot;Llama2 æœ‰å•†ç”¨è®¸å¯å—?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[<span class="subst">&#123;i&#125;</span>] <span class="subst">&#123;node.text&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[0] We release</p>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover</p>
<p>all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform</p>
<p>safety testing and tuning tailored to their specific applications of the model. We provide a responsible use</p>
<p>guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of</p>
<p>our responsible release strategy can be found in Section 5.3.</p>
<p>[1] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</p>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper</p>
<p>but are not releasing.Â§</p>
<ol>
<li>Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release</li>
</ol>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023).</p>
<p>[2] Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra</p>
<p>Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang</p>
<p>Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang</p>
<p>Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic</p>
<p>Sergey Edunov</p>
<p>Thomas Scialomâˆ—</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.</p>
<p>[3] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-</p>
<p>source models. Human raters judged model generations for safety violations across ~2,000 adversarial</p>
<p>prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is</p>
<p>important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the</p>
<p>prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these</p>
<p>safety evaluations are performed using content standards that are likely to be biased towards the Llama</p>
<p>2-Chat models.</p>
<p>We are releasing the following models to the general public for research and commercial useâ€¡:</p>
<ol>
<li>Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</li>
</ol>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters.</p>
<p>[4] These closed product LLMs are heavily fine-tuned to align with human</p>
<p>preferences, which greatly enhances their usability and safety. This step can require significant costs in</p>
<p>compute and human annotation, and is often not transparent or easily reproducible, limiting progress within</p>
<p>the community to advance AI alignment research.</p>
<p>In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and</p>
<p>Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,</p>
<p>Llama 2-Chat models generally perform better than existing open-source models. They also appear to</p>
<p>be on par with some of the closed-source models, at least on the human evaluations we performed (see</p>
<p>Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data</p>
<p>annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,</p>
<p>this paper contributes a thorough description of our fine-tuning methodology and approach to improving</p>
<p>LLM safety.</p>
</blockquote>
<p>ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼</p>
<p>å¯ä¸‹è½½å·¦ä¾§ rag_demo.py çš„å®Œæ•´ä¾‹å­åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.postprocessor <span class="keyword">import</span> SentenceTransformerRerank</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ£€ç´¢åæ’åºæ¨¡å‹</span></span><br><span class="line">postprocessor = SentenceTransformerRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-large&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes = postprocessor.postprocess_nodes(nodes, query_str=<span class="string">&quot;Llama2 æœ‰å•†ç”¨è®¸å¯å—?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[<span class="subst">&#123;i&#125;</span>] <span class="subst">&#123;node.text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>&#x2F;opt&#x2F;conda&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;tqdm&#x2F;auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See <a target="_blank" rel="noopener" href="https://ipywidgets.readthedocs.io/en/stable/user_install.html">https://ipywidgets.readthedocs.io/en/stable/user_install.html</a>  from .autonotebook import tqdm as notebook_tqdm &#x2F;home&#x2F;jovyan&#x2F;.local&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;huggingface_hub&#x2F;file_download.py:1132: FutureWarning: <code>resume_download</code> is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use <code>force_download=True</code>.  warnings.warn(</p>
<p>[0] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-</p>
<p>source models. Human raters judged model generations for safety violations across ~2,000 adversarial</p>
<p>prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is</p>
<p>important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the</p>
<p>prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these</p>
<p>safety evaluations are performed using content standards that are likely to be biased towards the Llama</p>
<p>2-Chat models.</p>
<p>We are releasing the following models to the general public for research and commercial useâ€¡:</p>
<ol>
<li>Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</li>
</ol>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters.</p>
<p>[1] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</p>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper</p>
<p>but are not releasing.Â§</p>
<ol>
<li>Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release</li>
</ol>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023).</p>
</blockquote>
<p>æ›´å¤šçš„ Rerank åŠå…¶å®ƒåå¤„ç†æ–¹æ³•ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/">Node Postprocessor Modules</a></p>
<h2 id="6-ç”Ÿæˆå›å¤ï¼ˆQA-Chatï¼‰"><a href="#6-ç”Ÿæˆå›å¤ï¼ˆQA-Chatï¼‰" class="headerlink" title="6 ç”Ÿæˆå›å¤ï¼ˆQA &amp; Chatï¼‰"></a>6 ç”Ÿæˆå›å¤ï¼ˆQA &amp; Chatï¼‰</h2><h3 id="6-1-å•è½®é—®ç­”ï¼ˆQuery-Engineï¼‰"><a href="#6-1-å•è½®é—®ç­”ï¼ˆQuery-Engineï¼‰" class="headerlink" title="6.1 å•è½®é—®ç­”ï¼ˆQuery Engineï¼‰"></a>6.1 å•è½®é—®ç­”ï¼ˆQuery Engineï¼‰</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine()</span><br><span class="line">response = qa_engine.query(<span class="string">&quot;Llama2 æœ‰å¤šå°‘å‚æ•°?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 æœ‰7B, 13B, å’Œ 70B å‚æ•°ã€‚</p>
</blockquote>
<p><strong>æµå¼è¾“å‡º</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine(streaming=<span class="literal">True</span>)</span><br><span class="line">response = qa_engine.query(<span class="string">&quot;Llama2 æœ‰å¤šå°‘å‚æ•°?&quot;</span>)</span><br><span class="line">response.print_response_stream()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 æœ‰7B, 13B, å’Œ 70B å‚æ•°ã€‚</p>
</blockquote>
<h3 id="6-2-å¤šè½®å¯¹è¯ï¼ˆChat-Engineï¼‰"><a href="#6-2-å¤šè½®å¯¹è¯ï¼ˆChat-Engineï¼‰" class="headerlink" title="6.2 å¤šè½®å¯¹è¯ï¼ˆChat Engineï¼‰"></a>6.2 å¤šè½®å¯¹è¯ï¼ˆChat Engineï¼‰</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">response = chat_engine.chat(<span class="string">&quot;Llama2 æœ‰å¤šå°‘å‚æ•°?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama2 æœ‰7B, 13B, å’Œ 70B å‚æ•°ã€‚</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = chat_engine.chat(<span class="string">&quot;How many at most?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama2 æœ€å¤šæœ‰70Bå‚æ•°ã€‚</p>
</blockquote>
<p><strong>æµå¼è¾“å‡º</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">streaming_response = chat_engine.stream_chat(<span class="string">&quot;Llama 2æœ‰å¤šå°‘å‚æ•°?&quot;</span>)</span><br><span class="line"><span class="comment"># streaming_response.print_response_stream()</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> streaming_response.response_gen:</span><br><span class="line">    <span class="built_in">print</span>(token, end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2æœ‰7B, 13B, å’Œ70Bå‚æ•°ã€‚</p>
</blockquote>
<h2 id="7-åº•å±‚æ¥å£ï¼šPromptã€LLM-ä¸-Embedding"><a href="#7-åº•å±‚æ¥å£ï¼šPromptã€LLM-ä¸-Embedding" class="headerlink" title="7 åº•å±‚æ¥å£ï¼šPromptã€LLM ä¸ Embedding"></a>7 åº•å±‚æ¥å£ï¼šPromptã€LLM ä¸ Embedding</h2><h3 id="7-1-Prompt-æ¨¡æ¿"><a href="#7-1-Prompt-æ¨¡æ¿" class="headerlink" title="7.1 Prompt æ¨¡æ¿"></a>7.1 Prompt æ¨¡æ¿</h3><p><code>PromptTemplate</code> å®šä¹‰æç¤ºè¯æ¨¡æ¿</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(<span class="string">&quot;å†™ä¸€ä¸ªå…³äº&#123;topic&#125;çš„ç¬‘è¯&quot;</span>)</span><br><span class="line"></span><br><span class="line">prompt.<span class="built_in">format</span>(topic=<span class="string">&quot;å°æ˜&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>â€˜å†™ä¸€ä¸ªå…³äºå°æ˜çš„ç¬‘è¯â€™</p>
</blockquote>
<p><code>ChatPromptTemplate</code> å®šä¹‰å¤šè½®æ¶ˆæ¯æ¨¡æ¿</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.llms <span class="keyword">import</span> ChatMessage, MessageRole</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">chat_text_qa_msgs = [</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.SYSTEM,</span><br><span class="line">        content=<span class="string">&quot;ä½ å«&#123;name&#125;ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.USER, </span><br><span class="line">        content=(</span><br><span class="line">            <span class="string">&quot;å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\n&quot;</span> \</span><br><span class="line">            <span class="string">&quot;&#123;context&#125;\n\n&quot;</span> \</span><br><span class="line">            <span class="string">&quot;é—®é¢˜ï¼š&#123;question&#125;&quot;</span></span><br><span class="line">        )</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    text_qa_template.<span class="built_in">format</span>(</span><br><span class="line">        name=<span class="string">&quot;ç“œç“œ&quot;</span>,</span><br><span class="line">        context=<span class="string">&quot;è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•&quot;</span>,</span><br><span class="line">        question=<span class="string">&quot;è¿™æ˜¯ä»€ä¹ˆ&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>system: ä½ å«ç“œç“œï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚</p>
<p>user: å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š</p>
<p>è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•</p>
<p>é—®é¢˜ï¼šè¿™æ˜¯ä»€ä¹ˆ</p>
<p>assistant:</p>
</blockquote>
<h3 id="7-2-è¯­è¨€æ¨¡å‹"><a href="#7-2-è¯­è¨€æ¨¡å‹" class="headerlink" title="7.2 è¯­è¨€æ¨¡å‹"></a>7.2 è¯­è¨€æ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br><span class="line"></span><br><span class="line">response = llm.complete(prompt.<span class="built_in">format</span>(topic=<span class="string">&quot;å°æ˜&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>æœ‰ä¸€å¤©ï¼Œå°æ˜åœ¨è¯¾å ‚ä¸Šå¬è€å¸ˆè®²è§£æ•°å­¦é¢˜ã€‚è€å¸ˆé—®é“ï¼šâ€œå¦‚æœä½ æœ‰10ä¸ªè‹¹æœï¼Œç»™äº†å°å3ä¸ªï¼Œç»™äº†å°çº¢2ä¸ªï¼Œä½ è¿˜å‰©ä¸‹å‡ ä¸ªè‹¹æœï¼Ÿâ€</p>
<p>å°æ˜æƒ³äº†æƒ³ï¼Œå›ç­”é“ï¼šâ€œè€å¸ˆï¼Œæˆ‘è¿˜å‰©ä¸‹5ä¸ªè‹¹æœã€‚â€</p>
<p>è€å¸ˆç‚¹ç‚¹å¤´ï¼Œç»§ç»­é—®ï¼šâ€œé‚£å¦‚æœä½ å†ç»™å°åˆš1ä¸ªè‹¹æœå‘¢ï¼Ÿâ€</p>
<p>å°æ˜çš±äº†çš±çœ‰å¤´ï¼Œè®¤çœŸåœ°è¯´ï¼šâ€œé‚£æˆ‘å°±å¾—å»ä¹°æ›´å¤šçš„è‹¹æœäº†ï¼â€</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">response = llm.complete(</span><br><span class="line">    text_qa_template.<span class="built_in">format</span>(</span><br><span class="line">        name=<span class="string">&quot;ç“œç“œ&quot;</span>,</span><br><span class="line">        context=<span class="string">&quot;è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•&quot;</span>,</span><br><span class="line">        question=<span class="string">&quot;ä½ æ˜¯è°ï¼Œæˆ‘ä»¬åœ¨å¹²å˜›&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>æˆ‘æ˜¯ç“œç“œï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€ä¸ªæµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®¾ç½®å…¨å±€ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">Settings.llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>é™¤ OpenAI å¤–ï¼ŒLlamaIndex å·²é›†æˆå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œæœ¬åœ°éƒ¨ç½² APIï¼Œè¯¦è§å®˜æ–¹æ–‡æ¡£ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/">Available LLM integrations</a></p>
<h3 id="7-3-Embedding-æ¨¡å‹"><a href="#7-3-Embedding-æ¨¡å‹" class="headerlink" title="7.3 Embedding æ¨¡å‹"></a>7.3 Embedding æ¨¡å‹</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># å…¨å±€è®¾å®š</span></span><br><span class="line">Settings.embed_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>, dimensions=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>

<p>LlamaIndex åŒæ ·é›†æˆäº†å¤šç§ Embedding æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œå¼€æºæ¨¡å‹ï¼ˆHuggingFaceï¼‰ç­‰ï¼Œè¯¦è§<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/">å®˜æ–¹æ–‡æ¡£</a>ã€‚</p>
<h2 id="8-åŸºäº-LlamaIndex-å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„-RAG-ç³»ç»Ÿ"><a href="#8-åŸºäº-LlamaIndex-å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„-RAG-ç³»ç»Ÿ" class="headerlink" title="8 åŸºäº LlamaIndex å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„ RAG ç³»ç»Ÿ"></a>8 åŸºäº LlamaIndex å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„ RAG ç³»ç»Ÿ</h2><p>åŠŸèƒ½è¦æ±‚ï¼š</p>
<ul>
<li>åŠ è½½æŒ‡å®šç›®å½•çš„æ–‡ä»¶</li>
<li>æ”¯æŒ RAG-Fusion</li>
<li>ä½¿ç”¨ ChromaDB å‘é‡æ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°</li>
<li>æ”¯æŒæ£€ç´¢åæ’åº</li>
<li>æ”¯æŒå¤šè½®å¯¹è¯</li>
</ul>
<p>ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ rag_demo.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"> </span><br><span class="line"><span class="comment"># åˆ›å»º ChromaDB å‘é‡æ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°</span></span><br><span class="line">chroma_client = chromadb.PersistentClient(path=<span class="string">&quot;./chroma_db&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, KeywordTableIndex, SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.core.ingestion <span class="keyword">import</span> IngestionPipeline</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.core.postprocessor <span class="keyword">import</span> SentenceTransformerRerank</span><br><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> QueryFusionRetriever</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.core.chat_engine <span class="keyword">import</span> CondenseQuestionChatEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> nest_asyncio</span><br><span class="line">nest_asyncio.apply() <span class="comment"># åªåœ¨Jupyterç¬”è®°ç¯å¢ƒä¸­éœ€è¦æ­¤æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. æŒ‡å®šå…¨å±€llmä¸embeddingæ¨¡å‹</span></span><br><span class="line">Settings.llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br><span class="line">Settings.embed_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>, dimensions=<span class="number">512</span>)</span><br><span class="line"><span class="comment"># 2. æŒ‡å®šå…¨å±€æ–‡æ¡£å¤„ç†çš„ Ingestion Pipeline</span></span><br><span class="line">Settings.transformations = [SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. åŠ è½½æœ¬åœ°æ–‡æ¡£</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>, file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. æ–°å»º collection</span></span><br><span class="line">collection_name = <span class="built_in">hex</span>(<span class="built_in">int</span>(time.time()))</span><br><span class="line">chroma_collection = chroma_client.get_or_create_collection(collection_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. åˆ›å»º Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"><span class="comment"># 6. æŒ‡å®š Vector Store çš„ Storage ç”¨äº index</span></span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=vector_store)</span><br><span class="line">index = VectorStoreIndex.from_documents(</span><br><span class="line">    documents, storage_context=storage_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. å®šä¹‰æ£€ç´¢åæ’åºæ¨¡å‹</span></span><br><span class="line">reranker = SentenceTransformerRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-large&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. å®šä¹‰ RAG Fusion æ£€ç´¢å™¨</span></span><br><span class="line">fusion_retriever = QueryFusionRetriever(</span><br><span class="line">    [index.as_retriever()],</span><br><span class="line">    similarity_top_k=<span class="number">5</span>, <span class="comment"># æ£€ç´¢å¬å› top k ç»“æœ</span></span><br><span class="line">    num_queries=<span class="number">3</span>,  <span class="comment"># ç”Ÿæˆ query æ•°</span></span><br><span class="line">    use_async=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># query_gen_prompt=&quot;...&quot;,  # å¯ä»¥è‡ªå®šä¹‰ query ç”Ÿæˆçš„ prompt æ¨¡æ¿</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. æ„å»ºå•è½® query engine</span></span><br><span class="line">query_engine = RetrieverQueryEngine.from_args(</span><br><span class="line">    fusion_retriever,</span><br><span class="line">    node_postprocessors=[reranker]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10. å¯¹è¯å¼•æ“</span></span><br><span class="line">chat_engine = CondenseQuestionChatEngine.from_defaults(</span><br><span class="line">    query_engine=query_engine, </span><br><span class="line">    <span class="comment"># condense_question_prompt=... # å¯ä»¥è‡ªå®šä¹‰ chat message prompt æ¨¡æ¿</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    question=<span class="built_in">input</span>(<span class="string">&quot;User:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> question.strip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    response = chat_engine.chat(question)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;AI: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>User: llama2 æœ‰å¤šå°‘å‚æ•°</p>
<p>AI: Llama 2 æœ‰ 7Bã€13B å’Œ 70B å‚æ•°çš„å˜ä½“ã€‚</p>
<p>User: æœ€å¤šå¤šå°‘</p>
<p>AI: Llama 2 çš„å˜ä½“ä¸­å‚æ•°æœ€å¤šæ˜¯ 70Bã€‚</p>
<p>User: ChatALLåœ¨å“ªä¸‹è½½</p>
</blockquote>
<h2 id="9-LlamaIndex-çš„æ›´å¤šåŠŸèƒ½"><a href="#9-LlamaIndex-çš„æ›´å¤šåŠŸèƒ½" class="headerlink" title="9 LlamaIndex çš„æ›´å¤šåŠŸèƒ½"></a>9 LlamaIndex çš„æ›´å¤šåŠŸèƒ½</h2><ul>
<li>æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¼€å‘æ¡†æ¶ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/</a></li>
<li>RAG çš„è¯„æµ‹ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/evaluating/">https://docs.llamaindex.ai/en/stable/module_guides/evaluating/</a></li>
<li>è¿‡ç¨‹ç›‘æ§ï¼š<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/observability/">https://docs.llamaindex.ai/en/stable/module_guides/observability/</a></li>
</ul>
<p>ä»¥ä¸Šå†…å®¹æ¶‰åŠè¾ƒå¤šèƒŒæ™¯çŸ¥è¯†ï¼Œæš‚æ—¶ä¸åœ¨æœ¬è¯¾å±•å¼€ï¼Œç›¸å…³çŸ¥è¯†ä¼šåœ¨åé¢è¯¾ç¨‹ä¸­é€ä¸€è¯¦ç»†è®²è§£ã€‚</p>
<p>æ­¤å¤–ï¼ŒLlamaIndex é’ˆå¯¹ç”Ÿäº§çº§çš„ RAG ç³»ç»Ÿä¸­é‡åˆ°çš„å„ä¸ªæ–¹é¢çš„ç»†èŠ‚é—®é¢˜ï¼Œæ€»ç»“äº†å¾ˆå¤šé«˜ç«¯æŠ€å·§ï¼ˆ<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/optimizing/production_rag/">Advanced Topics</a>ï¼‰ï¼Œå¯¹å®æˆ˜å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼Œéå¸¸æ¨èæœ‰èƒ½åŠ›çš„åŒå­¦é˜…è¯»ã€‚</p>

                
            </div>
            <hr/>

            



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">èµ</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">ä½ çš„èµè¯†æ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">æ”¯ä»˜å®</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">å¾® ä¿¡</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="æ”¯ä»˜å®æ‰“èµäºŒç»´ç ">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="å¾®ä¿¡æ‰“èµäºŒç»´ç ">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2025/03/26/07-LangChain/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="07-LangChain">
                        
                        <span class="card-title">07-LangChain</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Tang
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/03/26/05-Assistant-API/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="05-Assistant API">
                        
                        <span class="card-title">05-Assistant API</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Tang
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/about" target="_blank">Tang</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;Total visits:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;Total visitors:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/TangCharlotte/AI-Classes" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:485480375@qq.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=485480375" class="tooltipped" target="_blank" data-tooltip="QQè”ç³»æˆ‘: 485480375" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/bu-yan-92-91" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/bu-yan-92-91" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS è®¢é˜…" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
