<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="06-LlamaIndex, Blog">
    <meta name="description" content="1 大语言模型开发框架的价值SDK：Software Development Kit**，它是一组软件工具和资源的集合，旨在帮助开发者创建、测试、部署和维护应用程序或软件。
所有开发框架（SDK）的核心价值，都是降低开发、维护成本。
大语言">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>06-LlamaIndex | Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>Contact</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Blog</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/TangCharlotte/AI-Classes" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/TangCharlotte/AI-Classes" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">06-LlamaIndex</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2025-03-26
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="1-大语言模型开发框架的价值"><a href="#1-大语言模型开发框架的价值" class="headerlink" title="1 大语言模型开发框架的价值"></a>1 大语言模型开发框架的价值</h2><p><em>SDK<strong>：</strong>Software Development Kit**，它是一组软件工具和资源的集合，旨在帮助开发者创建、测试、部署和维护应用程序或软件。</em></p>
<p>所有开发框架（SDK）的核心价值，都是降低开发、维护成本。</p>
<p>大语言模型开发框架的价值，是让开发者可以更方便地开发基于大语言模型的应用。主要提供两类帮助：</p>
<ol>
<li>第三方能力抽象。比如 LLM、向量数据库、搜索接口等</li>
<li>常用工具、方案封装</li>
<li>底层实现封装。比如流式接口、超时重连、异步与并行等</li>
</ol>
<p>好的开发框架，需要具备以下特点：</p>
<ol>
<li>可靠性、鲁棒性高</li>
<li>可维护性高</li>
<li>可扩展性高</li>
<li>学习成本低</li>
</ol>
<p>举些通俗的例子：</p>
<ul>
<li>与外部功能解依赖<ul>
<li>比如可以随意更换 LLM 而不用大量重构代码</li>
<li>更换三方工具也同理</li>
</ul>
</li>
<li>经常变的部分要在外部维护而不是放在代码里<ul>
<li>比如 Prompt 模板</li>
</ul>
</li>
<li>各种环境下都适用<ul>
<li>比如线程安全</li>
</ul>
</li>
<li>方便调试和测试<ul>
<li>至少要能感觉到用了比不用方便吧</li>
<li>合法的输入不会引发框架内部的报错</li>
</ul>
</li>
</ul>
<p><strong>划重点：</strong>选对了框架，事半功倍；反之，事倍功半。</p>
<p><strong>什么是</strong> <strong>SDK</strong><strong>?</strong> <a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/what-is/sdk/">https://aws.amazon.com/cn/what-is/sdk/</a> <strong>SDK 和</strong> <strong>API</strong> <strong>的区别是什么?</strong> <a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/">https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/</a></p>
<p><strong>🌰 举个例子：使用</strong> <strong>SDK****，4 行代码实现一个简易的 RAG 系统</strong></p>
<p>运行本课代码前，请先重启一下 kernel，以重置所有配置。 </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">!pip install --upgrade llama-index</span><br><span class="line"></span><br><span class="line">from llama_index.core import VectorStoreIndex<span class="punctuation">,</span> SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>).load_data()</span><br><span class="line">index = VectorStoreIndex.from_documents(documents)</span><br><span class="line"></span><br><span class="line">query_engine = index.as_query_engine()</span><br><span class="line"></span><br><span class="line">response = query_engine.query(<span class="string">&quot;llama2有多少参数&quot;</span>)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 ranges in scale from 7 billion to 70 billion parameters.</p>
</blockquote>
<h2 id="2-LlamaIndex-介绍"><a href="#2-LlamaIndex-介绍" class="headerlink" title="2 LlamaIndex 介绍"></a>2 LlamaIndex 介绍</h2><p><em>「 LlamaIndex is a framework for building context-augmented</em> <em>LLM</em> <em>applications. Context augmentation refers to any use case that applies</em> <em>LLMs</em> <em>on top of your private or domain-specific data. 」</em></p>
<p>LlamaIndex 是一个为开发「上下文增强」的大语言模型应用的框架（也就是 SDK）。<strong>上下文增强</strong>，泛指任何在私有或特定领域数据基础上应用大语言模型的情况。例如：</p>
<img src="/2025/03/26/06-LlamaIndex/1742964772814-2.png" class="">

<ul>
<li>Question-Answering Chatbots (也就是 RAG)</li>
<li>Document Understanding and Extraction （文档理解与信息抽取）</li>
<li>Autonomous Agents that can perform research and take actions （智能体应用）</li>
</ul>
<p>LlamaIndex 有 Python 和 Typescript 两个版本，Python 版的文档相对更完善。</p>
<ul>
<li>Python 文档地址：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a></li>
<li>Python API 接口文档：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/">https://docs.llamaindex.ai/en/stable/api_reference/</a></li>
<li>TS 文档地址：<a target="_blank" rel="noopener" href="https://ts.llamaindex.ai/">https://ts.llamaindex.ai/</a></li>
<li>TS API 接口文档：<a target="_blank" rel="noopener" href="https://ts.llamaindex.ai/api/">https://ts.llamaindex.ai/api/</a></li>
</ul>
<p>LlamaIndex 是一个开源框架，Github 链接：<a target="_blank" rel="noopener" href="https://github.com/run-llama">https://github.com/run-llama</a></p>
<h3 id="2-1-LlamaIndex-的核心模块"><a href="#2-1-LlamaIndex-的核心模块" class="headerlink" title="2.1 LlamaIndex 的核心模块"></a>2.1 LlamaIndex 的核心模块</h3><img src="/2025/03/26/06-LlamaIndex/1742964772814-1.png" class="">

<h3 id="2-2-安装-LlamaIndex"><a href="#2-2-安装-LlamaIndex" class="headerlink" title="2.2 安装 LlamaIndex"></a>2.2 安装 LlamaIndex</h3><ol>
<li>Python</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install llama-index</span><br></pre></td></tr></table></figure>

<ol>
<li>Typescript</li>
</ol>
<p>通过 npm 安装 npm install llamaindex 通过 yarn 安装 yarn add llamaindex 通过 pnpm 安装 pnpm add llamaindex</p>
<p>本课程以 Python 版为例进行讲解。</p>
<h2 id="3-数据加载（Loading）"><a href="#3-数据加载（Loading）" class="headerlink" title="3 数据加载（Loading）"></a>3 数据加载（Loading）</h2><h3 id="3-1-加载本地数据"><a href="#3-1-加载本地数据" class="headerlink" title="3.1 加载本地数据"></a>3.1 加载本地数据</h3><p><code>SimpleDirectoryReader</code> 是一个简单的本地文件加载器。它会遍历指定目录，并根据文件扩展名自动加载文件（<strong>文本内容</strong>）。</p>
<p>支持的文件类型：</p>
<ul>
<li><code>.csv</code> - comma-separated values</li>
<li><code>.docx</code> - Microsoft Word</li>
<li><code>.epub</code> - EPUB ebook format</li>
<li><code>.hwp</code> - Hangul Word Processor</li>
<li><code>.ipynb</code> - Jupyter Notebook</li>
<li><code>.jpeg</code>, <code>.jpg</code> - JPEG image</li>
<li><code>.mbox</code> - MBOX email archive</li>
<li><code>.md</code> - Markdown</li>
<li><code>.mp3</code>, <code>.mp4</code> - audio and video</li>
<li><code>.pdf</code> - Portable Document Format</li>
<li><code>.png</code> - Portable Network Graphics</li>
<li><code>.ppt</code>, <code>.pptm</code>, <code>.pptx</code> - Microsoft PowerPoint</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> pydantic.v1 <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_json</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于展示json数据&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, <span class="built_in">str</span>):</span><br><span class="line">        obj = json.loads(data)</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(obj, indent=<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(data, <span class="built_in">dict</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>):</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(data, indent=<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">issubclass</span>(<span class="built_in">type</span>(data), BaseModel):</span><br><span class="line">        <span class="built_in">print</span>(json.dumps(data.<span class="built_in">dict</span>(), indent=<span class="number">4</span>, ensure_ascii=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_obj</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于展示一组对象&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">            show_json(item)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Input is not a list&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"></span><br><span class="line">reader = SimpleDirectoryReader(</span><br><span class="line">        input_dir=<span class="string">&quot;./data&quot;</span>, <span class="comment"># 目标目录</span></span><br><span class="line">        recursive=<span class="literal">False</span>, <span class="comment"># 是否递归遍历子目录</span></span><br><span class="line">        required_exts=[<span class="string">&quot;.pdf&quot;</span>] <span class="comment"># (可选)只读取指定后缀的文件</span></span><br><span class="line">    )</span><br><span class="line">documents = reader.load_data()</span><br><span class="line"></span><br><span class="line">show_json(documents[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>​    “id_”: “892804e2-9a5d-4853-b12d-2abae6621bfe”,</p>
<p>​    “embedding”: null,</p>
<p>​    “metadata”: {</p>
<p>​        “page_label”: “1”,</p>
<p>​        “file_name”: “llama2-extracted.pdf”,</p>
<p>​        “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​        “file_type”: “application&#x2F;pdf”,</p>
<p>​        “file_size”: 401338,</p>
<p>​        “creation_date”: “2024-06-14”,</p>
<p>​        “last_modified_date”: “2024-06-14”</p>
<p>​    },</p>
<p>​    “excluded_embed_metadata_keys”: [</p>
<p>​        “file_name”,</p>
<p>​        “file_type”,</p>
<p>​        “file_size”,</p>
<p>​        “creation_date”,</p>
<p>​        “last_modified_date”,</p>
<p>​        “last_accessed_date”</p>
<p>​    ],</p>
<p>​    “excluded_llm_metadata_keys”: [</p>
<p>​        “file_name”,</p>
<p>​        “file_type”,</p>
<p>​        “file_size”,</p>
<p>​        “creation_date”,</p>
<p>​        “last_modified_date”,</p>
<p>​        “last_accessed_date”</p>
<p>​    ],</p>
<p>​    “relationships”: {},</p>
<p>​    “text”: “Llama 2: OpenFoundation andFine-Tuned ChatModels\nHugo Touvron∗Louis Martin†Kevin Stone†\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra\nPrajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller\nCynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev\nPunit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich\nYinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra\nIgor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang\nRoss Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang\nAngela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic\nSergey Edunov ThomasScialom∗\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\nsource models. We provide a detailed description of our approach to fine-tuning and safety\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsibledevelopmentof LLMs.\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n†Second author\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023”,</p>
<p>​    “mimetype”: “text&#x2F;plain”,</p>
<p>​    “start_char_idx”: null,</p>
<p>​    “end_char_idx”: null,</p>
<p>​    “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​    “metadata_template”: “{key}: {value}”,</p>
<p>​    “metadata_seperator”: “\n”,</p>
<p>​    “class_name”: “Document”</p>
<p>}</p>
<p>Llama 2: OpenFoundation andFine-Tuned ChatModels</p>
<p>Hugo Touvron∗Louis Martin†Kevin Stone†</p>
<p>Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov SoumyaBatra</p>
<p>Prajjwal Bhargava Shruti Bhosale Dan Bikel LukasBlecher Cristian CantonFerrer MoyaChen</p>
<p>Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu BrianFuller</p>
<p>Cynthia Gao VedanujGoswami NamanGoyal AnthonyHartshorn Saghar Hosseini RuiHou</p>
<p>Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa IsabelKloumann ArtemKorenev</p>
<p>Punit Singh Koura Marie-AnneLachaux ThibautLavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu YuningMao Xavier Martinet Todor Mihaylov PushkarMishra</p>
<p>Igor Molybog Yixin Nie AndrewPoulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva EricMichael Smith Ranjan Subramanian XiaoqingEllenTan BinhTang</p>
<p>Ross Taylor AdinaWilliams JianXiang Kuan PuxinXu ZhengYan Iliyan Zarov YuchenZhang</p>
<p>Angela Fan MelanieKambadur SharanNarang Aurelien Rodriguez RobertStojnic</p>
<p>Sergey Edunov ThomasScialom∗</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our</p>
<p>models outperform open-source chat models on most benchmarks we tested, and based on</p>
<p>ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-</p>
<p>source models. We provide a detailed description of our approach to fine-tuning and safety</p>
<p>improvements of Llama 2-Chat in order to enable the community to build on our work and</p>
<p>contribute to the responsibledevelopmentof LLMs.</p>
<p>∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com</p>
<p>†Second author</p>
<p>Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023</p>
</blockquote>
<p><strong>注意：</strong>对图像、视频、语音类文件，默认不会自动提取其中文字。如需提取，参考下面介绍的 <code>Data Connectors</code>。</p>
<p>默认的 <code>PDFReader</code> 效果并不理想，我们可以更换文件加载器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install pymupdf</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"></span><br><span class="line">reader = SimpleDirectoryReader(</span><br><span class="line">        input_dir=<span class="string">&quot;./data&quot;</span>, <span class="comment"># 目标目录</span></span><br><span class="line">        recursive=<span class="literal">False</span>, <span class="comment"># 是否递归遍历子目录</span></span><br><span class="line">        required_exts=[<span class="string">&quot;.pdf&quot;</span>], <span class="comment"># (可选)只读取指定后缀的文件</span></span><br><span class="line">        file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125; <span class="comment"># 指定特定的文件加载器</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">documents = reader.load_data()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Hugo Touvron∗</p>
<p>Louis Martin†</p>
<p>Kevin Stone†</p>
<p>Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra</p>
<p>Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen</p>
<p>Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller</p>
<p>Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou</p>
<p>Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev</p>
<p>Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra</p>
<p>Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang</p>
<p>Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang</p>
<p>Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic</p>
<p>Sergey Edunov</p>
<p>Thomas Scialom∗</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our</p>
<p>models outperform open-source chat models on most benchmarks we tested, and based on</p>
<p>our human evaluations for helpfulness and safety, may be a suitable substitute for closed-</p>
<p>source models. We provide a detailed description of our approach to fine-tuning and safety</p>
<p>improvements of Llama 2-Chat in order to enable the community to build on our work and</p>
<p>contribute to the responsible development of LLMs.</p>
<p>∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com</p>
<p>†Second author</p>
<p>Contributions for all the authors can be found in Section A.1.</p>
<p>arXiv:2307.09288v2  [cs.CL]  19 Jul 2023</p>
</blockquote>
<p>更多的 PDF 加载器还有 <code>SmartPDFLoader</code> 和 <code>LlamaParse</code>, 二者都提供了更丰富的解析能力，包括解析章节与段落结构等。但不是 100%准确，偶有文字丢失或错位情况，建议根据自身需求详细测试评估。</p>
<h3 id="3-2-Data-Connectors"><a href="#3-2-Data-Connectors" class="headerlink" title="3.2 Data Connectors"></a>3.2 Data Connectors</h3><p>用于处理更丰富的数据类型，并将其读取为 <code>Document</code> 的形式（text + metadata）。</p>
<p>例如：加载一个<a target="_blank" rel="noopener" href="https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf">飞书文档</a>。（飞书文档 API 访问权限申请，请参考此<a target="_blank" rel="noopener" href="http://localhost:8888/files/07-llamaindex/%E9%A3%9E%E4%B9%A6%E6%96%87%E6%A1%A3%E7%9B%B8%E5%85%B3%E6%9D%83%E9%99%90%E7%94%B3%E8%AF%B7.pdf?_xsrf=2%7Cae0d2c71%7C210dc1c6729a7a86b8cdd1c4ac1722a3%7C1742464210">说明文档</a>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install llama-index-readers-feishu-docs</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.readers.feishu_docs <span class="keyword">import</span> FeishuDocsReader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 见说明文档</span></span><br><span class="line">app_id = <span class="string">&quot;cli_a6f1c0fa1fd9d00b&quot;</span></span><br><span class="line">app_secret = <span class="string">&quot;dMXCTy8DGaty2xn8I858ZbFDFvcqgiep&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://agiclass.feishu.cn/docx/FULadzkWmovlfkxSgLPcE4oWnPf</span></span><br><span class="line"><span class="comment"># 链接最后的 &quot;FULadzkWmovlfkxSgLPcE4oWnPf&quot; 为文档 ID </span></span><br><span class="line">doc_ids = [<span class="string">&quot;FULadzkWmovlfkxSgLPcE4oWnPf&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义飞书文档加载器</span></span><br><span class="line">loader = FeishuDocsReader(app_id, app_secret)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文档</span></span><br><span class="line">documents = loader.load_data(document_ids=doc_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示前1000字符</span></span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].text[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>AI 大模型全栈工程师培养计划 - AGIClass.ai</p>
<p>由 AGI 课堂推出的社群型会员制课程，传授大模型的原理、应用开发技术和行业认知，助你成为 ChatGPT 浪潮中的超级个体</p>
<p>什么是 AI 大模型全栈工程师？</p>
<p>「AI 大模型全栈工程师」简称「AI 全栈」，是一个人就能借助 AI，设计、开发和运营基于 AI 的大模型应用的超级个体。</p>
<p>AI 全栈需要懂业务、懂 AI、懂编程，一个人就是一个团队，单枪匹马创造财富。</p>
<p>在技术型公司，AI 全栈最懂 AI，瞬间站上技术顶峰。</p>
<p>在非技术型公司，AI 全栈连接其他员工和 AI，提升整个公司的效率。</p>
<p>在公司外，AI 全栈接项目，独立开发变现小工具，赚取丰厚副业收入。</p>
<p>适合人群</p>
<p>学习本课程，可以在下述目标中三选一：</p>
<p>成为 AI 全栈：懂业务、懂 AI 也懂编程。大量使用 AI，自己完成 AI 应用从策划、开发到落地的全过程。包括商业分析、需求分析、产品设计、开发、测试、市场推广和运营等</p>
<p>成为业务向 AI 全栈：懂业务也懂 AI，与程序员合作，一起完成 AI 应用从策划、开发到落地的全过程</p>
<p>成为编程向 AI 全栈：懂编程也懂 AI，与业务人员合作，一起完成 AI 应用从策划、开发到落地的全过程</p>
<p>懂至少一门编程语言，并有过真实项目开发经验的软件开发⼯程师、⾼级⼯程师、技术总监、研发经理、架构师、测试⼯程师、数据工程师、运维工程师等，建议以「AI 全栈」为目标。即便对商业、产品、市场等的学习达不到最佳，但已掌握的经验和认知也有助于成为有竞争力的「编程向AI 全栈」。</p>
<p>不懂编程的产品经理、需求分析师、创业者、老板、解决方案工程师、项目经理、运营、市场、销售、设计师等，建议优先选择「业务向 AI 全栈」为目标。在课程提供的技术环境里熏陶，提高技术领域的判断力，未来可以和技术人员更流畅地沟通协作。学习过程中，如果能善用 AI 学习编程、辅助编程，就可以向「AI 全栈」迈进。</p>
<p>XXX</p>
<p>image.png</p>
</blockquote>
<p><strong>更多 Data Connectors</strong></p>
<ul>
<li>内置的<a target="_blank" rel="noopener" href="https://llamahub.ai/l/readers/llama-index-readers-file">文件加载器</a></li>
<li>连接三方服务的<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/">数据加载器</a>，例如数据库</li>
<li>更多加载器可以在 <a target="_blank" rel="noopener" href="https://llamahub.ai/">LlamaHub</a> 上找到</li>
</ul>
<h2 id="4-文本切分与解析（Chunking）"><a href="#4-文本切分与解析（Chunking）" class="headerlink" title="4 文本切分与解析（Chunking）"></a>4 文本切分与解析（Chunking）</h2><p>为方便检索，我们通常把 <code>Document</code> 切分为 <code>Node</code>。</p>
<p>在 LlamaIndex 中，<code>Node</code> 被定义为一个文本的「chunk」。</p>
<h3 id="4-1-使用-TextSplitters-对文本做切分"><a href="#4-1-使用-TextSplitters-对文本做切分" class="headerlink" title="4.1 使用 TextSplitters 对文本做切分"></a>4.1 使用 TextSplitters 对文本做切分</h3><p>例如：<code>TokenTextSplitter</code> 按指定 token 数切分文本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> TokenTextSplitter</span><br><span class="line"></span><br><span class="line">node_parser = TokenTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">100</span>,  <span class="comment"># 每个 chunk 的最大长度</span></span><br><span class="line">    chunk_overlap=<span class="number">50</span>  <span class="comment"># chunk 之间重叠长度 </span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes = node_parser.get_nodes_from_documents(</span><br><span class="line">    documents, show_progress=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">show_json(nodes[<span class="number">0</span>])</span><br><span class="line">show_json(nodes[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>​    “id_”: “e29fecbc-961b-4881-9bfb-4714d9515b5c”,</p>
<p>​    “embedding”: null,</p>
<p>​    “metadata”: {</p>
<p>​        “document_id”: “FULadzkWmovlfkxSgLPcE4oWnPf”</p>
<p>​    },</p>
<p>​    “excluded_embed_metadata_keys”: [],</p>
<p>​    “excluded_llm_metadata_keys”: [],</p>
<p>​    “relationships”: {</p>
<p>​        “1”: {</p>
<p>​            “node_id”: “4d2992f6-1cab-440b-af2c-7b74f5f1152c”,</p>
<p>​            “node_type”: “4”,</p>
<p>​            “metadata”: {</p>
<p>​                “document_id”: “FULadzkWmovlfkxSgLPcE4oWnPf”</p>
<p>​            },</p>
<p>​            “hash”: “4af8c2ff953f76fa1d608b31dc95b87ee24474294c5e34b83f28902032f054af”,</p>
<p>​            “class_name”: “RelatedNodeInfo”</p>
<p>​        },</p>
<p>​        “3”: {</p>
<p>​            “node_id”: “7569cb43-e42b-4081-9a48-0ff8c90d6181”,</p>
<p>​            “node_type”: “1”,</p>
<p>​            “metadata”: {},</p>
<p>​            “hash”: “654c6cbdd5a23946a84e84e6f3a474de2a442191b2be2d817ba7f04286b1a980”,</p>
<p>​            “class_name”: “RelatedNodeInfo”</p>
<p>​        }</p>
<p>​    },</p>
<p>​    “text”: “AI 大模型全栈工程师培养计划 - AGIClass.ai\n\n由 AGI 课堂推出的社群型会员制课程，传授大模型的原理、应用开发技术和行业认知，助你成为”,</p>
<p>​    “mimetype”: “text&#x2F;plain”,</p>
<p>​    “start_char_idx”: 0,</p>
<p>​    “end_char_idx”: 76,</p>
<p>​    “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​    “metadata_template”: “{key}: {value}”,</p>
<p>​    “metadata_seperator”: “\n”,</p>
<p>​    “class_name”: “TextNode”</p>
<p>}</p>
<p>{</p>
<p>​    “id_”: “7569cb43-e42b-4081-9a48-0ff8c90d6181”,</p>
<p>​    “embedding”: null,</p>
<p>​    “metadata”: {</p>
<p>​        “document_id”: “FULadzkWmovlfkxSgLPcE4oWnPf”</p>
<p>​    },</p>
<p>​    “excluded_embed_metadata_keys”: [],</p>
<p>​    “excluded_llm_metadata_keys”: [],</p>
<p>​    “relationships”: {</p>
<p>​        “1”: {</p>
<p>​            “node_id”: “4d2992f6-1cab-440b-af2c-7b74f5f1152c”,</p>
<p>​            “node_type”: “4”,</p>
<p>​            “metadata”: {</p>
<p>​                “document_id”: “FULadzkWmovlfkxSgLPcE4oWnPf”</p>
<p>​            },</p>
<p>​            “hash”: “4af8c2ff953f76fa1d608b31dc95b87ee24474294c5e34b83f28902032f054af”,</p>
<p>​            “class_name”: “RelatedNodeInfo”</p>
<p>​        },</p>
<p>​        “2”: {</p>
<p>​            “node_id”: “e29fecbc-961b-4881-9bfb-4714d9515b5c”,</p>
<p>​            “node_type”: “1”,</p>
<p>​            “metadata”: {</p>
<p>​                “document_id”: “FULadzkWmovlfkxSgLPcE4oWnPf”</p>
<p>​            },</p>
<p>​            “hash”: “b08e60a1cf7fa55aa8c010d792766208dcbb34e58aeead16dca005eab4e1df8f”,</p>
<p>​            “class_name”: “RelatedNodeInfo”</p>
<p>​        },</p>
<p>​        “3”: {</p>
<p>​            “node_id”: “1d77241c-5d68-47b8-a475-a9793ca3397a”,</p>
<p>​            “node_type”: “1”,</p>
<p>​            “metadata”: {},</p>
<p>​            “hash”: “06d6c13287ff7e2f033a1aae487198dbfdec3d954aab0fd9b4866ce833200afb”,</p>
<p>​            “class_name”: “RelatedNodeInfo”</p>
<p>​        }</p>
<p>​    },</p>
<p>​    “text”: “AGI 课堂推出的社群型会员制课程，传授大模型的原理、应用开发技术和行业认知，助你成为 ChatGPT 浪潮中的超级个体\n什么是 AI”,</p>
<p>​    “mimetype”: “text&#x2F;plain”,</p>
<p>​    “start_char_idx”: 33,</p>
<p>​    “end_char_idx”: 100,</p>
<p>​    “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​    “metadata_template”: “{key}: {value}”,</p>
<p>​    “metadata_seperator”: “\n”,</p>
<p>​    “class_name”: “TextNode”</p>
<p>}</p>
</blockquote>
<p>LlamaIndex 提供了丰富的 <code>TextSplitter</code>，例如：</p>
<ul>
<li><code>SentenceSplitter</code>：在切分指定长度的 chunk 同时尽量保证句子边界不被切断；</li>
<li><code>CodeSplitter</code>：根据 AST（编译器的抽象句法树）切分代码，保证代码功能片段完整；</li>
<li><code>SemanticSplitterNodeParser</code>：根据语义相关性对将文本切分为片段。</li>
</ul>
<h3 id="4-2-使用-NodeParsers-对有结构的文档做解析"><a href="#4-2-使用-NodeParsers-对有结构的文档做解析" class="headerlink" title="4.2 使用 NodeParsers 对有结构的文档做解析"></a>4.2 使用 NodeParsers 对有结构的文档做解析</h3><p>例如：<code>MarkdownNodeParser</code>解析 markdown 文档</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> FlatReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> MarkdownNodeParser</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">md_docs = FlatReader().load_data(Path(<span class="string">&quot;./data/ChatALL.md&quot;</span>))</span><br><span class="line">parser = MarkdownNodeParser()</span><br><span class="line">nodes = parser.get_nodes_from_documents(md_docs)</span><br><span class="line"></span><br><span class="line">show_json(nodes[<span class="number">2</span>])</span><br><span class="line">show_json(nodes[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{    “id_”: “95fdd1ba-f376-423c-8e56-791b959f5427”,    “embedding”: null,    “metadata”: {        “filename”: “ChatALL.md”,        “extension”: “.md”,        “Header_2”: “功能”    },    “excluded_embed_metadata_keys”: [],    “excluded_llm_metadata_keys”: [],    “relationships”: {        “1”: {            “node_id”: “4a985a3f-cf0f-41bb-b3a4-eda18a1351ec”,            “node_type”: “4”,            “metadata”: {                “filename”: “ChatALL.md”,                “extension”: “.md”            },            “hash”: “45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87e”,            “class_name”: “RelatedNodeInfo”        },        “2”: {            “node_id”: “7a5e7373-f294-433f-b361-a9051af73938”,            “node_type”: “1”,            “metadata”: {                “filename”: “ChatALL.md”,                “extension”: “.md”,                “Header_2”: “屏幕截图”            },            “hash”: “f6065ad5e9929bc7ee14e3c4cc2d29c06788501df8887476c30b279ba8ffd594”,            “class_name”: “RelatedNodeInfo”        },        “3”: {            “node_id”: “ced63c8e-eda5-46e5-9d81-d9140a37ab92”,            “node_type”: “1”,            “metadata”: {                “Header_2”: “功能”,                “Header_3”: “这是你吗？”            },            “hash”: “f54ac07d417fbcbd606e7cdd3de28c30804e2213218dec2e6157d5037a23e289”,            “class_name”: “RelatedNodeInfo”        }    },    “text”: “功能\n\n基于大型语言模型（LLMs）的 AI 机器人非常神奇。然而，它们的行为可能是随机的，不同的机器人在不同的任务上表现也有差异。如果你想获得最佳体验，不要一个一个尝试。ChatALL（中文名：齐叨）可以把一条指令同时发给多个 AI，帮助您发现最好的回答。你需要做的只是<a target="_blank" rel="noopener" href="https://github.com/sunner/ChatALL/releases">下载、安装</a>和提问。”,    “mimetype”: “text&#x2F;plain”,    “start_char_idx”: 459,    “end_char_idx”: 650,    “text_template”: “{metadata_str}\n\n{content}”,    “metadata_template”: “{key}: {value}”,    “metadata_seperator”: “\n”,    “class_name”: “TextNode” } {    “id_”: “ced63c8e-eda5-46e5-9d81-d9140a37ab92”,    “embedding”: null,    “metadata”: {        “filename”: “ChatALL.md”,        “extension”: “.md”,        “Header_2”: “功能”,        “Header_3”: “这是你吗？”    },    “excluded_embed_metadata_keys”: [],    “excluded_llm_metadata_keys”: [],    “relationships”: {        “1”: {            “node_id”: “4a985a3f-cf0f-41bb-b3a4-eda18a1351ec”,            “node_type”: “4”,            “metadata”: {                “filename”: “ChatALL.md”,                “extension”: “.md”            },            “hash”: “45b9149e0039c1ef7fbbd74f96923875505cc77916de48734ba7767f6a16a87e”,            “class_name”: “RelatedNodeInfo”        },        “2”: {            “node_id”: “95fdd1ba-f376-423c-8e56-791b959f5427”,            “node_type”: “1”,            “metadata”: {                “filename”: “ChatALL.md”,                “extension”: “.md”,                “Header_2”: “功能”            },            “hash”: “90172566aa1795d0f9ac33c954d0b98fde63bf9176950d0ea38e87e4ab6563ed”,            “class_name”: “RelatedNodeInfo”        },        “3”: {            “node_id”: “4f4d8aeb-30ed-45c5-9292-4dc0edce16be”,            “node_type”: “1”,            “metadata”: {                “Header_2”: “功能”,                “Header_3”: “支持的 AI”            },            “hash”: “1b2b11abec9fc74b725b6c344f37d44736e8e991a3eebdbcfa4ab682506c7b2e”,            “class_name”: “RelatedNodeInfo”        }    },    “text”: “这是你吗？\n\nChatALL 的典型用户是：\n\n- 🤠 <strong>大模型重度玩家</strong> ，希望从大模型找到最好的答案，或者最好的创作\n- 🤓 <strong>大模型研究者</strong> ，直观比较各种大模型在不同领域的优劣\n- 😎 <strong>大模型应用开发者</strong> ，快速调试 prompt，寻找表现最佳的基础模型”,    “mimetype”: “text&#x2F;plain”,    “start_char_idx”: 656,    “end_char_idx”: 788,    “text_template”: “{metadata_str}\n\n{content}”,    “metadata_template”: “{key}: {value}”,    “metadata_seperator”: “\n”,    “class_name”: “TextNode” }</p>
</blockquote>
<p>更多的 <code>NodeParser</code> 包括 <code>HTMLNodeParser</code>，<code>JSONNodeParser</code>等等。</p>
<h2 id="5-索引（Indexing）与检索（Retrieval）"><a href="#5-索引（Indexing）与检索（Retrieval）" class="headerlink" title="5 索引（Indexing）与检索（Retrieval）"></a>5 索引（Indexing）与检索（Retrieval）</h2><p><strong>基础概念</strong>：在「检索」相关的上下文中，「索引」即<code>index</code>， 通常是指为了实现快速检索而设计的特定「数据结构」。</p>
<p>索引的具体原理与实现不是本课程的教学重点，感兴趣的同学可以参考：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Search_engine_indexing">传统索引</a>、<a target="_blank" rel="noopener" href="https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5">向量索引</a></p>
<h3 id="5-1-向量检索"><a href="#5-1-向量检索" class="headerlink" title="5.1 向量检索"></a>5.1 向量检索</h3><ol>
<li><code>SimpleVectorStore</code> 直接在内存中构建一个 Vector Store 并建索引</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> TokenTextSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 pdf 文档</span></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    <span class="string">&quot;./data&quot;</span>, </span><br><span class="line">    required_exts=[<span class="string">&quot;.pdf&quot;</span>],</span><br><span class="line">    file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;</span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Node Parser</span></span><br><span class="line">node_parser = TokenTextSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分文档</span></span><br><span class="line">nodes = node_parser.get_nodes_from_documents(documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建 index</span></span><br><span class="line">index = VectorStoreIndex(nodes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(</span><br><span class="line">    similarity_top_k=<span class="number">2</span> <span class="comment"># 返回前两个结果</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2有多少参数&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>{</p>
<p>​    “node”: {</p>
<p>​        “id_”: “4a6537d5-72de-4eec-a6ee-981b44396d79”,</p>
<p>​        “embedding”: null,</p>
<p>​        “metadata”: {</p>
<p>​            “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​            “file_name”: “llama2-extracted.pdf”,</p>
<p>​            “file_type”: “application&#x2F;pdf”,</p>
<p>​            “file_size”: 401338,</p>
<p>​            “creation_date”: “2024-06-14”,</p>
<p>​            “last_modified_date”: “2024-06-14”,</p>
<p>​            “total_pages”: 4,</p>
<p>​            “source”: “4”</p>
<p>​        },</p>
<p>​        “excluded_embed_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “excluded_llm_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “relationships”: {</p>
<p>​            “1”: {</p>
<p>​                “node_id”: “e1be7502-7883-45cf-986a-0c88ecd7bad1”,</p>
<p>​                “node_type”: “4”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “b29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519a”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “2”: {</p>
<p>​                “node_id”: “4ffd9047-f4a4-438c-8871-09673a8ac4d2”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “07108bd9b8afa14b2c31a05dbe825ddf1cf5ca4ddcc8bb87eb57ce03045e7bc7”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “3”: {</p>
<p>​                “node_id”: “7f63c496-88da-4db6-8362-a2694772d621”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {},</p>
<p>​                “hash”: “726b948dfd9e32d525760994bddb61dbfaba8a0d18d12a3e3a4f9504fbc208fd”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            }</p>
<p>​        },</p>
<p>​        “text”: “an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of”,</p>
<p>​        “mimetype”: “text&#x2F;plain”,</p>
<p>​        “start_char_idx”: 752,</p>
<p>​        “end_char_idx”: 1714,</p>
<p>​        “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​        “metadata_template”: “{key}: {value}”,</p>
<p>​        “metadata_seperator”: “\n”,</p>
<p>​        “class_name”: “TextNode”</p>
<p>​    },</p>
<p>​    “score”: 0.7901917550666981,</p>
<p>​    “class_name”: “NodeWithScore”</p>
<p>}</p>
<p>{</p>
<p>​    “node”: {</p>
<p>​        “id_”: “bc33a188-0147-447e-8137-a0caccf05970”,</p>
<p>​        “embedding”: null,</p>
<p>​        “metadata”: {</p>
<p>​            “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​            “file_name”: “llama2-extracted.pdf”,</p>
<p>​            “file_type”: “application&#x2F;pdf”,</p>
<p>​            “file_size”: 401338,</p>
<p>​            “creation_date”: “2024-06-14”,</p>
<p>​            “last_modified_date”: “2024-06-14”,</p>
<p>​            “total_pages”: 4,</p>
<p>​            “source”: “1”</p>
<p>​        },</p>
<p>​        “excluded_embed_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “excluded_llm_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “relationships”: {</p>
<p>​            “1”: {</p>
<p>​                “node_id”: “1bb809bb-d25f-4e50-b774-ccd7402da25c”,</p>
<p>​                “node_type”: “4”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “1”</p>
<p>​                },</p>
<p>​                “hash”: “3ebf9b8910125e57c9eea78794c6566c0a85081c97dbf7e83a65e5d791bcda57”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “2”: {</p>
<p>​                “node_id”: “4337b7c7-6f45-4d2f-aa31-6def3b07088d”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “1”</p>
<p>​                },</p>
<p>​                “hash”: “bf3806ddee17b6020e00e4e722a57980d0c5fc9f813ff437a756c8dd8f44e52f”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “3”: {</p>
<p>​                “node_id”: “c29e860f-5f91-4cb2-a9e3-f860a0eb5f7d”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {},</p>
<p>​                “hash”: “fe31f0ee71493cf072edea470642efdc2be2103b9deb19d3706be19e2d1fef9b”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            }</p>
<p>​        },</p>
<p>​        “text”: “Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialom∗\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute for”,</p>
<p>​        “mimetype”: “text&#x2F;plain”,</p>
<p>​        “start_char_idx”: 513,</p>
<p>​        “end_char_idx”: 1464,</p>
<p>​        “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​        “metadata_template”: “{key}: {value}”,</p>
<p>​        “metadata_seperator”: “\n”,</p>
<p>​        “class_name”: “TextNode”</p>
<p>​    },</p>
<p>​    “score”: 0.7890007200916708,</p>
<p>​    “class_name”: “NodeWithScore”</p>
<p>}</p>
</blockquote>
<p>LlamaIndex 默认的 Embedding 模型是 <code>OpenAIEmbedding(model=&quot;text-embedding-ada-002&quot;)</code>。</p>
<p>如何替换指定的 Embedding 模型见后面章节详解。</p>
<ol>
<li>使用自定义的 Vector Store，以 <code>Chroma</code> 为例：</li>
</ol>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install llama-index-vector-stores-chroma</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">if</span> os.environ.get(<span class="string">&#x27;CUR_ENV_IS_STUDENT&#x27;</span>,<span class="string">&#x27;false&#x27;</span>)==<span class="string">&#x27;true&#x27;</span>:</span><br><span class="line">    <span class="built_in">__import__</span>(<span class="string">&#x27;pysqlite3&#x27;</span>)</span><br><span class="line">    <span class="keyword">import</span> sys</span><br><span class="line">    sys.modules[<span class="string">&#x27;sqlite3&#x27;</span>]= sys.modules.pop(<span class="string">&#x27;pysqlite3&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> chromadb.config <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Chroma Client</span></span><br><span class="line"><span class="comment"># EphemeralClient 在内存创建；如果需要存盘，可以使用 PersistentClient</span></span><br><span class="line">chroma_client = chromadb.EphemeralClient(settings=Settings(allow_reset=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"></span><br><span class="line">chroma_client.reset() <span class="comment"># 为演示方便，实际不用每次 reset</span></span><br><span class="line">chroma_collection = chroma_client.create_collection(<span class="string">&quot;demo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Storage Context 是 Vector Store 的存储容器，用于存储文本、index、向量等数据</span></span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=vector_store)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 index：通过 Storage Context 关联到自定义的 Vector Store</span></span><br><span class="line">index = VectorStoreIndex(nodes, storage_context=storage_context)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2有多少参数&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results)</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>{</p>
<p>​    “node”: {</p>
<p>​        “id_”: “4a6537d5-72de-4eec-a6ee-981b44396d79”,</p>
<p>​        “embedding”: null,</p>
<p>​        “metadata”: {</p>
<p>​            “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​            “file_name”: “llama2-extracted.pdf”,</p>
<p>​            “file_type”: “application&#x2F;pdf”,</p>
<p>​            “file_size”: 401338,</p>
<p>​            “creation_date”: “2024-06-14”,</p>
<p>​            “last_modified_date”: “2024-06-14”,</p>
<p>​            “total_pages”: 4,</p>
<p>​            “source”: “4”</p>
<p>​        },</p>
<p>​        “excluded_embed_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “excluded_llm_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “relationships”: {</p>
<p>​            “1”: {</p>
<p>​                “node_id”: “e1be7502-7883-45cf-986a-0c88ecd7bad1”,</p>
<p>​                “node_type”: “4”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “b29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519a”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “2”: {</p>
<p>​                “node_id”: “4ffd9047-f4a4-438c-8871-09673a8ac4d2”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “07108bd9b8afa14b2c31a05dbe825ddf1cf5ca4ddcc8bb87eb57ce03045e7bc7”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “3”: {</p>
<p>​                “node_id”: “7f63c496-88da-4db6-8362-a2694772d621”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {},</p>
<p>​                “hash”: “726b948dfd9e32d525760994bddb61dbfaba8a0d18d12a3e3a4f9504fbc208fd”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            }</p>
<p>​        },</p>
<p>​        “text”: “an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover\nall scenarios. Therefore, before deploying any applications of”,</p>
<p>​        “mimetype”: “text&#x2F;plain”,</p>
<p>​        “start_char_idx”: 752,</p>
<p>​        “end_char_idx”: 1714,</p>
<p>​        “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​        “metadata_template”: “{key}: {value}”,</p>
<p>​        “metadata_seperator”: “\n”,</p>
<p>​        “class_name”: “TextNode”</p>
<p>​    },</p>
<p>​    “score”: 0.657386283435787,</p>
<p>​    “class_name”: “NodeWithScore”</p>
<p>}</p>
<p>{</p>
<p>​    “node”: {</p>
<p>​        “id_”: “bc33a188-0147-447e-8137-a0caccf05970”,</p>
<p>​        “embedding”: null,</p>
<p>​        “metadata”: {</p>
<p>​            “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​            “file_name”: “llama2-extracted.pdf”,</p>
<p>​            “file_type”: “application&#x2F;pdf”,</p>
<p>​            “file_size”: 401338,</p>
<p>​            “creation_date”: “2024-06-14”,</p>
<p>​            “last_modified_date”: “2024-06-14”,</p>
<p>​            “total_pages”: 4,</p>
<p>​            “source”: “1”</p>
<p>​        },</p>
<p>​        “excluded_embed_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “excluded_llm_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “relationships”: {</p>
<p>​            “1”: {</p>
<p>​                “node_id”: “1bb809bb-d25f-4e50-b774-ccd7402da25c”,</p>
<p>​                “node_type”: “4”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “1”</p>
<p>​                },</p>
<p>​                “hash”: “3ebf9b8910125e57c9eea78794c6566c0a85081c97dbf7e83a65e5d791bcda57”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “2”: {</p>
<p>​                “node_id”: “4337b7c7-6f45-4d2f-aa31-6def3b07088d”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “1”</p>
<p>​                },</p>
<p>​                “hash”: “bf3806ddee17b6020e00e4e722a57980d0c5fc9f813ff437a756c8dd8f44e52f”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “3”: {</p>
<p>​                “node_id”: “c29e860f-5f91-4cb2-a9e3-f860a0eb5f7d”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {},</p>
<p>​                “hash”: “fe31f0ee71493cf072edea470642efdc2be2103b9deb19d3706be19e2d1fef9b”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            }</p>
<p>​        },</p>
<p>​        “text”: “Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\nSergey Edunov\nThomas Scialom∗\nGenAI, Meta\nAbstract\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\nOur fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\nmodels outperform open-source chat models on most benchmarks we tested, and based on\nour human evaluations for helpfulness and safety, may be a suitable substitute for”,</p>
<p>​        “mimetype”: “text&#x2F;plain”,</p>
<p>​        “start_char_idx”: 513,</p>
<p>​        “end_char_idx”: 1464,</p>
<p>​        “text_template”: “{metadata_str}\n\n{content}”,</p>
<p>​        “metadata_template”: “{key}: {value}”,</p>
<p>​        “metadata_seperator”: “\n”,</p>
<p>​        “class_name”: “TextNode”</p>
<p>​    },</p>
<p>​    “score”: 0.6557053381809197,</p>
<p>​    “class_name”: “NodeWithScore”</p>
<p>}</p>
</blockquote>
<h3 id="5-2-更多索引与检索方式"><a href="#5-2-更多索引与检索方式" class="headerlink" title="5.2 更多索引与检索方式"></a>5.2 更多索引与检索方式</h3><p>LlamaIndex 内置了丰富的检索机制，例如：</p>
<ul>
<li>关键字检索<ul>
<li><code>BM25Retriever</code>：基于 tokenizer 实现的 BM25 经典检索算法</li>
<li><code>KeywordTableGPTRetriever</code>：使用 GPT 提取检索关键字</li>
<li><code>KeywordTableSimpleRetriever</code>：使用正则表达式提取检索关键字</li>
<li><code>KeywordTableRAKERetriever</code>：使用<code>RAKE</code>算法提取检索关键字（有语言限制）</li>
</ul>
</li>
<li>RAG-Fusion <code>QueryFusionRetriever</code></li>
<li>还支持 <a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/">KnowledgeGraph</a>、<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever">SQL</a>、<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever">Text-to-SQL</a> 等等</li>
</ul>
<h3 id="5-3-Ingestion-Pipeline-自定义数据处理流程"><a href="#5-3-Ingestion-Pipeline-自定义数据处理流程" class="headerlink" title="5.3 Ingestion Pipeline 自定义数据处理流程"></a>5.3 Ingestion Pipeline 自定义数据处理流程</h3><p>LlamaIndex 通过 <code>Transformations</code> 定义一个数据（<code>Documents</code>）的多步处理的流程（Pipeline）。 这个 Pipeline 的一个显著特点是，<strong>它的每个子步骤是可以缓存（cache）的</strong>，即如果该子步骤的输入与处理方法不变，重复调用时会直接从缓存中获取结果，而无需重新执行该子步骤，这样即节省时间也会节省 token （如果子步骤涉及大模型调用）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.start = time.time()</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, exc_type, exc_val, exc_tb</span>):</span><br><span class="line">        <span class="variable language_">self</span>.end = time.time()</span><br><span class="line">        <span class="variable language_">self</span>.interval = <span class="variable language_">self</span>.end - <span class="variable language_">self</span>.start</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;耗时 <span class="subst">&#123;self.interval*<span class="number">1000</span>&#125;</span> ms&quot;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.core.extractors <span class="keyword">import</span> TitleExtractor</span><br><span class="line"><span class="keyword">from</span> llama_index.core.ingestion <span class="keyword">import</span> IngestionPipeline</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"><span class="keyword">import</span> nest_asyncio</span><br><span class="line">nest_asyncio.apply() <span class="comment"># 只在Jupyter笔记环境中需要此操作，否则会报错</span></span><br><span class="line"></span><br><span class="line">chroma_client.reset() <span class="comment"># 为演示方便，实际不用每次 reset</span></span><br><span class="line">chroma_collection = chroma_client.create_collection(<span class="string">&quot;ingestion_demo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"></span><br><span class="line">pipeline = IngestionPipeline(</span><br><span class="line">    transformations=[</span><br><span class="line">        SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>), <span class="comment"># 按句子切分</span></span><br><span class="line">        TitleExtractor(), <span class="comment"># 利用 LLM 对文本生成标题</span></span><br><span class="line">        OpenAIEmbedding(), <span class="comment"># 将文本向量化</span></span><br><span class="line">    ],</span><br><span class="line">    vector_store=vector_store,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">documents = SimpleDirectoryReader(</span><br><span class="line">    <span class="string">&quot;./data&quot;</span>, </span><br><span class="line">    required_exts=[<span class="string">&quot;.pdf&quot;</span>],</span><br><span class="line">    file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;</span><br><span class="line">).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计时</span></span><br><span class="line"><span class="keyword">with</span> Timer():</span><br><span class="line">    <span class="comment"># Ingest directly into a vector db</span></span><br><span class="line">    pipeline.run(documents=documents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line">index = VectorStoreIndex.from_vector_store(vector_store)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索</span></span><br><span class="line">results = vector_retriever.retrieve(<span class="string">&quot;Llama2有多少参数&quot;</span>)</span><br><span class="line"></span><br><span class="line">show_list_obj(results[:<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>100%|██████████| 3&#x2F;3 [00:00&lt;00:00,  4.56it&#x2F;s]</p>
<p>100%|██████████| 5&#x2F;5 [00:01&lt;00:00,  4.97it&#x2F;s]</p>
<p>100%|██████████| 5&#x2F;5 [00:01&lt;00:00,  4.78it&#x2F;s]</p>
<p>100%|██████████| 4&#x2F;4 [00:00&lt;00:00,  6.43it&#x2F;s]</p>
<p>耗时 6928.267955780029 ms</p>
<p>{</p>
<p>​    “node”: {</p>
<p>​        “id_”: “bae00644-0188-4e5e-a0df-4b6342585815”,</p>
<p>​        “embedding”: null,</p>
<p>​        “metadata”: {</p>
<p>​            “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​            “file_name”: “llama2-extracted.pdf”,</p>
<p>​            “file_type”: “application&#x2F;pdf”,</p>
<p>​            “file_size”: 401338,</p>
<p>​            “creation_date”: “2024-06-14”,</p>
<p>​            “last_modified_date”: “2024-06-14”,</p>
<p>​            “total_pages”: 4,</p>
<p>​            “source”: “4”,</p>
<p>​            “document_title”: “Responsible Release and Deployment Strategy for Llama 2 and Llama 2-Chat Models”</p>
<p>​        },</p>
<p>​        “excluded_embed_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “excluded_llm_metadata_keys”: [</p>
<p>​            “file_name”,</p>
<p>​            “file_type”,</p>
<p>​            “file_size”,</p>
<p>​            “creation_date”,</p>
<p>​            “last_modified_date”,</p>
<p>​            “last_accessed_date”</p>
<p>​        ],</p>
<p>​        “relationships”: {</p>
<p>​            “1”: {</p>
<p>​                “node_id”: “9921e324-4f4c-4b9e-92cd-e3aae69a7ca0”,</p>
<p>​                “node_type”: “4”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “b29837a2e4d3e1e2b226990cb3eb14138fc66be2a51372a68641272c2095519a”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “2”: {</p>
<p>​                “node_id”: “b38e93ce-156f-4615-bd56-0a51eaa276d2”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {</p>
<p>​                    “file_path”: “&#x2F;home&#x2F;jovyan&#x2F;lecture-notes&#x2F;07-llamaindex&#x2F;data&#x2F;llama2-extracted.pdf”,</p>
<p>​                    “file_name”: “llama2-extracted.pdf”,</p>
<p>​                    “file_type”: “application&#x2F;pdf”,</p>
<p>​                    “file_size”: 401338,</p>
<p>​                    “creation_date”: “2024-06-14”,</p>
<p>​                    “last_modified_date”: “2024-06-14”,</p>
<p>​                    “total_pages”: 4,</p>
<p>​                    “source”: “4”</p>
<p>​                },</p>
<p>​                “hash”: “9d81d8fc1b12d06f9209d238abdd84d2e44083be69c925f28443906d62356482”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            },</p>
<p>​            “3”: {</p>
<p>​                “node_id”: “c7a204d3-fdd0-42c4-b191-58f43e6bb80e”,</p>
<p>​                “node_type”: “1”,</p>
<p>​                “metadata”: {},</p>
<p>​                “hash”: “84d9afeda9c20e1ede5c3a74fd65c9f1a15c2b124ba73de9369264ecddfbd169”,</p>
<p>​                “class_name”: “RelatedNodeInfo”</p>
<p>​            }</p>
<p>​        },</p>
<p>​        “text”: “Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also\nincreased the size of the pretraining corpus by 40%, doubled the context length of the model, and\nadopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with\n7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper\nbut are not releasing.§\n2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\nvariants of this model with 7B, 13B, and 70B parameters as well.\nWe believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,\nLlama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;\nSolaiman et al., 2023).”,</p>
<p>​        “mimetype”: “text&#x2F;plain”,</p>
<p>​        “start_char_idx”: 743,</p>
<p>​        “end_char_idx”: 1569,</p>
<p>​        “text_template”: “[Excerpt from document]\n{metadata_str}\nExcerpt:\n—–\n{content}\n—–\n”,</p>
<p>​        “metadata_template”: “{key}: {value}”,</p>
<p>​        “metadata_seperator”: “\n”,</p>
<p>​        “class_name”: “TextNode”</p>
<p>​    },</p>
<p>​    “score”: 0.6505982749190239,</p>
<p>​    “class_name”: “NodeWithScore”</p>
<p>}</p>
</blockquote>
<p>本地保存 <code>IngestionPipeline</code> 的缓存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pipeline.persist(<span class="string">&quot;./pipeline_storage&quot;</span>)</span><br><span class="line"></span><br><span class="line">new_pipeline = IngestionPipeline(</span><br><span class="line">    transformations=[</span><br><span class="line">        SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>),</span><br><span class="line">        TitleExtractor(),</span><br><span class="line">        OpenAIEmbedding()</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载缓存</span></span><br><span class="line">new_pipeline.load(<span class="string">&quot;./pipeline_storage&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Timer():</span><br><span class="line">    nodes = new_pipeline.run(documents=documents)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.20it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  3.07it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.67it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.77it&#x2F;s]</p>
<p>100%|██████████| 5&#x2F;5 [00:00&lt;00:00,  5.59it&#x2F;s]</p>
<p>100%|██████████| 2&#x2F;2 [00:00&lt;00:00,  2.09it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.33it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.86it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.65it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.89it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.12it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.52it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.29it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.42it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.46it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.44it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.31it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.97it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  1.98it&#x2F;s]</p>
<p>100%|██████████| 1&#x2F;1 [00:00&lt;00:00,  2.12it&#x2F;s]</p>
<p>耗时 22366.679430007935 ms</p>
</blockquote>
<p>此外，也可以用远程的 Redis 或 MongoDB 等存储 <code>IngestionPipeline</code> 的缓存，具体参考官方文档：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#remote-cache-management">Remote Cache Management</a>。</p>
<p><code>IngestionPipeline</code> 也支持异步和并发调用，请参考官方文档：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#async-support">Async Support</a>、<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/#parallel-processing">Parallel Processing</a>。</p>
<h3 id="5-4-检索后处理"><a href="#5-4-检索后处理" class="headerlink" title="5.4 检索后处理"></a>5.4 检索后处理</h3><p>LlamaIndex 的 <code>Node Postprocessors</code> 提供了一系列检索后处理模块。</p>
<p>例如：我们可以用不同模型对检索后的 <code>Nodes</code> 做重排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取 retriever</span></span><br><span class="line">vector_retriever = index.as_retriever(similarity_top_k=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索</span></span><br><span class="line">nodes = vector_retriever.retrieve(<span class="string">&quot;Llama2 有商用许可吗?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[<span class="subst">&#123;i&#125;</span>] <span class="subst">&#123;node.text&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[0] We release</p>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover</p>
<p>all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform</p>
<p>safety testing and tuning tailored to their specific applications of the model. We provide a responsible use</p>
<p>guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of</p>
<p>our responsible release strategy can be found in Section 5.3.</p>
<p>[1] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</p>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper</p>
<p>but are not releasing.§</p>
<ol>
<li>Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release</li>
</ol>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023).</p>
<p>[2] Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich</p>
<p>Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra</p>
<p>Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi</p>
<p>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang</p>
<p>Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang</p>
<p>Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic</p>
<p>Sergey Edunov</p>
<p>Thomas Scialom∗</p>
<p>GenAI, Meta</p>
<p>Abstract</p>
<p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned</p>
<p>large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.</p>
<p>Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.</p>
<p>[3] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-</p>
<p>source models. Human raters judged model generations for safety violations across ~2,000 adversarial</p>
<p>prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is</p>
<p>important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the</p>
<p>prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these</p>
<p>safety evaluations are performed using content standards that are likely to be biased towards the Llama</p>
<p>2-Chat models.</p>
<p>We are releasing the following models to the general public for research and commercial use‡:</p>
<ol>
<li>Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</li>
</ol>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters.</p>
<p>[4] These closed product LLMs are heavily fine-tuned to align with human</p>
<p>preferences, which greatly enhances their usability and safety. This step can require significant costs in</p>
<p>compute and human annotation, and is often not transparent or easily reproducible, limiting progress within</p>
<p>the community to advance AI alignment research.</p>
<p>In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and</p>
<p>Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,</p>
<p>Llama 2-Chat models generally perform better than existing open-source models. They also appear to</p>
<p>be on par with some of the closed-source models, at least on the human evaluations we performed (see</p>
<p>Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data</p>
<p>annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,</p>
<p>this paper contributes a thorough description of our fine-tuning methodology and approach to improving</p>
<p>LLM safety.</p>
</blockquote>
<p>以下代码不要在服务器上运行，会死机！</p>
<p>可下载左侧 rag_demo.py 的完整例子在自己本地运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.postprocessor <span class="keyword">import</span> SentenceTransformerRerank</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索后排序模型</span></span><br><span class="line">postprocessor = SentenceTransformerRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-large&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">nodes = postprocessor.postprocess_nodes(nodes, query_str=<span class="string">&quot;Llama2 有商用许可吗?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[<span class="subst">&#123;i&#125;</span>] <span class="subst">&#123;node.text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>&#x2F;opt&#x2F;conda&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;tqdm&#x2F;auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See <a target="_blank" rel="noopener" href="https://ipywidgets.readthedocs.io/en/stable/user_install.html">https://ipywidgets.readthedocs.io/en/stable/user_install.html</a>  from .autonotebook import tqdm as notebook_tqdm &#x2F;home&#x2F;jovyan&#x2F;.local&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;huggingface_hub&#x2F;file_download.py:1132: FutureWarning: <code>resume_download</code> is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use <code>force_download=True</code>.  warnings.warn(</p>
<p>[0] Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed-</p>
<p>source models. Human raters judged model generations for safety violations across ~2,000 adversarial</p>
<p>prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is</p>
<p>important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the</p>
<p>prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these</p>
<p>safety evaluations are performed using content standards that are likely to be biased towards the Llama</p>
<p>2-Chat models.</p>
<p>We are releasing the following models to the general public for research and commercial use‡:</p>
<ol>
<li>Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</li>
</ol>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters.</p>
<p>[1] Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also</p>
<p>increased the size of the pretraining corpus by 40%, doubled the context length of the model, and</p>
<p>adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with</p>
<p>7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper</p>
<p>but are not releasing.§</p>
<ol>
<li>Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release</li>
</ol>
<p>variants of this model with 7B, 13B, and 70B parameters as well.</p>
<p>We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,</p>
<p>Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021;</p>
<p>Solaiman et al., 2023).</p>
</blockquote>
<p>更多的 Rerank 及其它后处理方法，参考官方文档：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/">Node Postprocessor Modules</a></p>
<h2 id="6-生成回复（QA-Chat）"><a href="#6-生成回复（QA-Chat）" class="headerlink" title="6 生成回复（QA &amp; Chat）"></a>6 生成回复（QA &amp; Chat）</h2><h3 id="6-1-单轮问答（Query-Engine）"><a href="#6-1-单轮问答（Query-Engine）" class="headerlink" title="6.1 单轮问答（Query Engine）"></a>6.1 单轮问答（Query Engine）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine()</span><br><span class="line">response = qa_engine.query(<span class="string">&quot;Llama2 有多少参数?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 有7B, 13B, 和 70B 参数。</p>
</blockquote>
<p><strong>流式输出</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qa_engine = index.as_query_engine(streaming=<span class="literal">True</span>)</span><br><span class="line">response = qa_engine.query(<span class="string">&quot;Llama2 有多少参数?&quot;</span>)</span><br><span class="line">response.print_response_stream()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2 有7B, 13B, 和 70B 参数。</p>
</blockquote>
<h3 id="6-2-多轮对话（Chat-Engine）"><a href="#6-2-多轮对话（Chat-Engine）" class="headerlink" title="6.2 多轮对话（Chat Engine）"></a>6.2 多轮对话（Chat Engine）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">response = chat_engine.chat(<span class="string">&quot;Llama2 有多少参数?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama2 有7B, 13B, 和 70B 参数。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">response = chat_engine.chat(<span class="string">&quot;How many at most?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama2 最多有70B参数。</p>
</blockquote>
<p><strong>流式输出</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chat_engine = index.as_chat_engine()</span><br><span class="line">streaming_response = chat_engine.stream_chat(<span class="string">&quot;Llama 2有多少参数?&quot;</span>)</span><br><span class="line"><span class="comment"># streaming_response.print_response_stream()</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> streaming_response.response_gen:</span><br><span class="line">    <span class="built_in">print</span>(token, end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Llama 2有7B, 13B, 和70B参数。</p>
</blockquote>
<h2 id="7-底层接口：Prompt、LLM-与-Embedding"><a href="#7-底层接口：Prompt、LLM-与-Embedding" class="headerlink" title="7 底层接口：Prompt、LLM 与 Embedding"></a>7 底层接口：Prompt、LLM 与 Embedding</h2><h3 id="7-1-Prompt-模板"><a href="#7-1-Prompt-模板" class="headerlink" title="7.1 Prompt 模板"></a>7.1 Prompt 模板</h3><p><code>PromptTemplate</code> 定义提示词模板</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(<span class="string">&quot;写一个关于&#123;topic&#125;的笑话&quot;</span>)</span><br><span class="line"></span><br><span class="line">prompt.<span class="built_in">format</span>(topic=<span class="string">&quot;小明&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>‘写一个关于小明的笑话’</p>
</blockquote>
<p><code>ChatPromptTemplate</code> 定义多轮消息模板</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core.llms <span class="keyword">import</span> ChatMessage, MessageRole</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line">chat_text_qa_msgs = [</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.SYSTEM,</span><br><span class="line">        content=<span class="string">&quot;你叫&#123;name&#125;，你必须根据用户提供的上下文回答问题。&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    ChatMessage(</span><br><span class="line">        role=MessageRole.USER, </span><br><span class="line">        content=(</span><br><span class="line">            <span class="string">&quot;已知上下文：\n&quot;</span> \</span><br><span class="line">            <span class="string">&quot;&#123;context&#125;\n\n&quot;</span> \</span><br><span class="line">            <span class="string">&quot;问题：&#123;question&#125;&quot;</span></span><br><span class="line">        )</span><br><span class="line">    ),</span><br><span class="line">]</span><br><span class="line">text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    text_qa_template.<span class="built_in">format</span>(</span><br><span class="line">        name=<span class="string">&quot;瓜瓜&quot;</span>,</span><br><span class="line">        context=<span class="string">&quot;这是一个测试&quot;</span>,</span><br><span class="line">        question=<span class="string">&quot;这是什么&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>system: 你叫瓜瓜，你必须根据用户提供的上下文回答问题。</p>
<p>user: 已知上下文：</p>
<p>这是一个测试</p>
<p>问题：这是什么</p>
<p>assistant:</p>
</blockquote>
<h3 id="7-2-语言模型"><a href="#7-2-语言模型" class="headerlink" title="7.2 语言模型"></a>7.2 语言模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br><span class="line"></span><br><span class="line">response = llm.complete(prompt.<span class="built_in">format</span>(topic=<span class="string">&quot;小明&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>有一天，小明在课堂上听老师讲解数学题。老师问道：“如果你有10个苹果，给了小华3个，给了小红2个，你还剩下几个苹果？”</p>
<p>小明想了想，回答道：“老师，我还剩下5个苹果。”</p>
<p>老师点点头，继续问：“那如果你再给小刚1个苹果呢？”</p>
<p>小明皱了皱眉头，认真地说：“那我就得去买更多的苹果了！”</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">response = llm.complete(</span><br><span class="line">    text_qa_template.<span class="built_in">format</span>(</span><br><span class="line">        name=<span class="string">&quot;瓜瓜&quot;</span>,</span><br><span class="line">        context=<span class="string">&quot;这是一个测试&quot;</span>,</span><br><span class="line">        question=<span class="string">&quot;你是谁，我们在干嘛&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>我是瓜瓜，我们正在进行一个测试。</p>
</blockquote>
<p><strong>设置全局使用的语言模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line">Settings.llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>除 OpenAI 外，LlamaIndex 已集成多个大语言模型，包括云服务 API 和本地部署 API，详见官方文档：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/">Available LLM integrations</a></p>
<h3 id="7-3-Embedding-模型"><a href="#7-3-Embedding-模型" class="headerlink" title="7.3 Embedding 模型"></a>7.3 Embedding 模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局设定</span></span><br><span class="line">Settings.embed_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>, dimensions=<span class="number">512</span>)</span><br></pre></td></tr></table></figure>

<p>LlamaIndex 同样集成了多种 Embedding 模型，包括云服务 API 和开源模型（HuggingFace）等，详见<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/">官方文档</a>。</p>
<h2 id="8-基于-LlamaIndex-实现一个功能较完整的-RAG-系统"><a href="#8-基于-LlamaIndex-实现一个功能较完整的-RAG-系统" class="headerlink" title="8 基于 LlamaIndex 实现一个功能较完整的 RAG 系统"></a>8 基于 LlamaIndex 实现一个功能较完整的 RAG 系统</h2><p>功能要求：</p>
<ul>
<li>加载指定目录的文件</li>
<li>支持 RAG-Fusion</li>
<li>使用 ChromaDB 向量数据库，并持久化到本地</li>
<li>支持检索后排序</li>
<li>支持多轮对话</li>
</ul>
<p>以下代码不要在服务器上运行，会死机！可下载左侧 rag_demo.py 在自己本地运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建 ChromaDB 向量数据库，并持久化到本地</span></span><br><span class="line">chroma_client = chromadb.PersistentClient(path=<span class="string">&quot;./chroma_db&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> VectorStoreIndex, KeywordTableIndex, SimpleDirectoryReader</span><br><span class="line"><span class="keyword">from</span> llama_index.vector_stores.chroma <span class="keyword">import</span> ChromaVectorStore</span><br><span class="line"><span class="keyword">from</span> llama_index.core.node_parser <span class="keyword">import</span> SentenceSplitter</span><br><span class="line"><span class="keyword">from</span> llama_index.core.ingestion <span class="keyword">import</span> IngestionPipeline</span><br><span class="line"><span class="keyword">from</span> llama_index.readers.file <span class="keyword">import</span> PyMuPDFReader</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> Settings</span><br><span class="line"><span class="keyword">from</span> llama_index.core <span class="keyword">import</span> StorageContext</span><br><span class="line"><span class="keyword">from</span> llama_index.core.postprocessor <span class="keyword">import</span> SentenceTransformerRerank</span><br><span class="line"><span class="keyword">from</span> llama_index.core.retrievers <span class="keyword">import</span> QueryFusionRetriever</span><br><span class="line"><span class="keyword">from</span> llama_index.core.query_engine <span class="keyword">import</span> RetrieverQueryEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.core.chat_engine <span class="keyword">import</span> CondenseQuestionChatEngine</span><br><span class="line"><span class="keyword">from</span> llama_index.llms.openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> llama_index.embeddings.openai <span class="keyword">import</span> OpenAIEmbedding</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> nest_asyncio</span><br><span class="line">nest_asyncio.apply() <span class="comment"># 只在Jupyter笔记环境中需要此操作，否则会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 指定全局llm与embedding模型</span></span><br><span class="line">Settings.llm = OpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o&quot;</span>)</span><br><span class="line">Settings.embed_model = OpenAIEmbedding(model=<span class="string">&quot;text-embedding-3-small&quot;</span>, dimensions=<span class="number">512</span>)</span><br><span class="line"><span class="comment"># 2. 指定全局文档处理的 Ingestion Pipeline</span></span><br><span class="line">Settings.transformations = [SentenceSplitter(chunk_size=<span class="number">300</span>, chunk_overlap=<span class="number">100</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 加载本地文档</span></span><br><span class="line">documents = SimpleDirectoryReader(<span class="string">&quot;./data&quot;</span>, file_extractor=&#123;<span class="string">&quot;.pdf&quot;</span>: PyMuPDFReader()&#125;).load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 新建 collection</span></span><br><span class="line">collection_name = <span class="built_in">hex</span>(<span class="built_in">int</span>(time.time()))</span><br><span class="line">chroma_collection = chroma_client.get_or_create_collection(collection_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 创建 Vector Store</span></span><br><span class="line">vector_store = ChromaVectorStore(chroma_collection=chroma_collection)</span><br><span class="line"><span class="comment"># 6. 指定 Vector Store 的 Storage 用于 index</span></span><br><span class="line">storage_context = StorageContext.from_defaults(vector_store=vector_store)</span><br><span class="line">index = VectorStoreIndex.from_documents(</span><br><span class="line">    documents, storage_context=storage_context</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 定义检索后排序模型</span></span><br><span class="line">reranker = SentenceTransformerRerank(</span><br><span class="line">    model=<span class="string">&quot;BAAI/bge-reranker-large&quot;</span>, top_n=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 定义 RAG Fusion 检索器</span></span><br><span class="line">fusion_retriever = QueryFusionRetriever(</span><br><span class="line">    [index.as_retriever()],</span><br><span class="line">    similarity_top_k=<span class="number">5</span>, <span class="comment"># 检索召回 top k 结果</span></span><br><span class="line">    num_queries=<span class="number">3</span>,  <span class="comment"># 生成 query 数</span></span><br><span class="line">    use_async=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># query_gen_prompt=&quot;...&quot;,  # 可以自定义 query 生成的 prompt 模板</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. 构建单轮 query engine</span></span><br><span class="line">query_engine = RetrieverQueryEngine.from_args(</span><br><span class="line">    fusion_retriever,</span><br><span class="line">    node_postprocessors=[reranker]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10. 对话引擎</span></span><br><span class="line">chat_engine = CondenseQuestionChatEngine.from_defaults(</span><br><span class="line">    query_engine=query_engine, </span><br><span class="line">    <span class="comment"># condense_question_prompt=... # 可以自定义 chat message prompt 模板</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    question=<span class="built_in">input</span>(<span class="string">&quot;User:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> question.strip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    response = chat_engine.chat(question)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;AI: <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>User: llama2 有多少参数</p>
<p>AI: Llama 2 有 7B、13B 和 70B 参数的变体。</p>
<p>User: 最多多少</p>
<p>AI: Llama 2 的变体中参数最多是 70B。</p>
<p>User: ChatALL在哪下载</p>
</blockquote>
<h2 id="9-LlamaIndex-的更多功能"><a href="#9-LlamaIndex-的更多功能" class="headerlink" title="9 LlamaIndex 的更多功能"></a>9 LlamaIndex 的更多功能</h2><ul>
<li>智能体（Agent）开发框架：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/</a></li>
<li>RAG 的评测：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/evaluating/">https://docs.llamaindex.ai/en/stable/module_guides/evaluating/</a></li>
<li>过程监控：<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/module_guides/observability/">https://docs.llamaindex.ai/en/stable/module_guides/observability/</a></li>
</ul>
<p>以上内容涉及较多背景知识，暂时不在本课展开，相关知识会在后面课程中逐一详细讲解。</p>
<p>此外，LlamaIndex 针对生产级的 RAG 系统中遇到的各个方面的细节问题，总结了很多高端技巧（<a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/optimizing/production_rag/">Advanced Topics</a>），对实战很有参考价值，非常推荐有能力的同学阅读。</p>

                
            </div>
            <hr/>

            



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2025/03/26/07-LangChain/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="07-LangChain">
                        
                        <span class="card-title">07-LangChain</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Tang
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/03/26/05-Assistant-API/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="05-Assistant API">
                        
                        <span class="card-title">05-Assistant API</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Tang
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('4'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/about" target="_blank">Tang</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;Total visits:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;Total visitors:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/TangCharlotte/AI-Classes" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:485480375@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=485480375" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 485480375" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>





    <a href="https://www.zhihu.com/people/bu-yan-92-91" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/bu-yan-92-91" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
